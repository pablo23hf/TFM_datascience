{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "438adc99-d934-4f7d-bd35-57ae605554f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7084f3-db8d-4c57-948c-2ba6e133e9b4",
   "metadata": {},
   "source": [
    "## Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b327f3-690c-4f92-b07d-51f7984cf8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverseDir(root):\n",
    "    for (dirpath, dirnames, filenames) in os.walk(root):\n",
    "        for file in filenames:\n",
    "            if file.endswith(('.nc')):\n",
    "                yield os.path.join(dirpath, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "985eb6dd-6358-4fb5-8bde-c155e6a6d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/jovyan/shared/data/students/pablo23hf/ERA5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc05ea2b-f0e6-4408-91fe-6eed741446d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = np.sort(list(traverseDir(root)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ed71133-e102-48bf-8d60-947e2fa1626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hourly = xr.open_mfdataset(files[0:72], \n",
    "                       concat_dim='time',\n",
    "                       combine = 'nested',\n",
    "                       chunks={\"time\":100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54ec28c0-6b8a-46cd-92ab-290d0b256a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorto dominio para que sea cuadrado y mÃºltiplo de 2\n",
    "#n_cells=48\n",
    "n_cells=8\n",
    "lon_bnds= ds_hourly.longitude[10:10+n_cells]\n",
    "lat_bnds= ds_hourly.latitude[0:0+n_cells]\n",
    "ds_hourly = ds_hourly.sel(longitude = lon_bnds, latitude = lat_bnds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c01b6-3a4a-4cd4-9a7b-f7920f753dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dc8bcfa-3ffb-4285-bb1d-7ae050ef72c3",
   "metadata": {},
   "source": [
    "## Generar los archivos .npz y .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d17f80c4-ef94-4524-86f3-cb50180b5dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  5.7s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "\n",
    "#PARAMS\n",
    "# for training data:\n",
    "startdate = '19700101'\n",
    "enddate = '19741231'\n",
    "# for test data:\n",
    "# startdate = '19750101'\n",
    "# enddate = '19751231'\n",
    "\n",
    "outpath='/home/jovyan/work/prueba_datos_ERA5'\n",
    "ds_hourly['tp'] = ds_hourly['tp']*1000\n",
    "ds_hourly=ds_hourly['tp']\n",
    "# convert to 32bit\n",
    "ds_hourly = ds_hourly.astype('float32')\n",
    "\n",
    "\n",
    "# convert to numpy array\n",
    "ds_hourly = ds_hourly.values\n",
    "\n",
    "# now we want to reshape to (days,tperday,lat,lon)\n",
    "t_per_day = int(24/1)\n",
    "\n",
    "ntime,ny,nx = ds_hourly.shape\n",
    "ndays = ntime / t_per_day\n",
    "assert(ndays.is_integer())\n",
    "ndays = int(ndays)\n",
    "reshaped = ds_hourly.reshape((ndays,t_per_day,ny,nx))\n",
    "\n",
    "final = reshaped\n",
    "\n",
    "#np.savez_compressed(f'{outpath}/prueba.npz',data=final)\n",
    "#np.save(f'{outpath}/prueba', final)\n",
    "np.savez_compressed(f'{outpath}/prueba{startdate}-{enddate}.npz',data=final)\n",
    "np.save(f'{outpath}/prueba{startdate}-{enddate}', final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d346c-0507-4c37-abe7-3c6f357ee929",
   "metadata": {},
   "source": [
    "## Valid training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "443e1581-bc04-40eb-9f9d-3e7d0bdc9a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 0 valid samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import numba\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "\n",
    "os.system('mkdir -p data')\n",
    "\n",
    "#PARAMS\n",
    "# for training data:\n",
    "startdate = '19700101'\n",
    "enddate = '19741231'\n",
    "# for test data:\n",
    "# startdate = '19750101'\n",
    "# enddate = '19751231'\n",
    "\n",
    "ndomain = n_cells  # gridpoints\n",
    "stride = n_cells  # |ndomain # in which steps to scan the whole domain\n",
    "\n",
    "tp_thresh_daily = 5  # mm. in the radardate the unit is mm/h, but then on 5 minutes steps.\n",
    "# the conversion is done automatically in this script\n",
    "n_thresh = 20\n",
    "# END PARAMS\n",
    "\n",
    "#if ndomain % 2 != 0:\n",
    "#    raise ValueError(f'ndomain must be an even number')\n",
    "\n",
    "datapath = '/home/jovyan/work/prueba_datos_ERA5'\n",
    "\n",
    "#ifile = f'/home/jovyan/work/prueba_datos_ERA5/prueba.npy'\n",
    "ifile = f'{datapath}/prueba{startdate}-{enddate}.npy'\n",
    "\n",
    "data = np.load(ifile, mmap_mode='r')\n",
    "\n",
    "if len(data.shape) != 4:\n",
    "    raise ValueError(f'data has wrong number of dimensions {len(data.shape)} instead of 4')\n",
    "\n",
    "# compute daily sum, which is the sum over the hour axis\n",
    "n_days,nhour, ny, nx = data.shape\n",
    "\n",
    "# compute all valid indices\n",
    "# for this, we try out all ndomain x ndomain squares shifted by strides, and check whether they have any missing data,\n",
    "# and if not, whether they adhere to the criteria set by tp_thresh_daily and n_thresh\n",
    "# since this contains many for loops, we speed it up with numba\n",
    "\n",
    "\n",
    "@numba.jit\n",
    "def filter(data):\n",
    "    final_valid_idcs = []\n",
    "    # loop over timeslices\n",
    "    for tidx in numba.prange(n_days):\n",
    "        #print(tidx, '/', n_days)\n",
    "        # daily sum\n",
    "        sub = np.sum(data[tidx],axis=0)\n",
    "        # loop over all possible boxes\n",
    "        for ii in range(0, ny - ndomain, stride):\n",
    "            for jj in range(0, nx - ndomain, stride):\n",
    "                subsub = sub[ii:ii + ndomain, jj:jj + ndomain]\n",
    "                # check for nan values\n",
    "                if not np.any(np.isnan(subsub)):\n",
    "                    # if at least n_thresh points are above the threshold,\n",
    "                    # we use this box\n",
    "                    if np.sum(subsub > tp_thresh_daily) >= n_thresh:\n",
    "                        final_valid_idcs.append((tidx, ii, jj))\n",
    "\n",
    "\n",
    "    return final_valid_idcs\n",
    "\n",
    "\n",
    "final_valid_idcs = filter(data)\n",
    "\n",
    "\n",
    "pickle.dump(final_valid_idcs, open(f'/home/jovyan/work/prueba_datos_ERA5/valid_indices_ERA5-{startdate}-{enddate}.pkl', 'wb'))\n",
    "\n",
    "print(f'found {len(final_valid_idcs)} valid samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dd436c0-6649-4262-819e-68eba5961228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           8.92207026e-04, 8.92207026e-04, 2.49594450e-02],\n",
       "          [5.34765422e-03, 8.02241266e-03, 1.15875155e-02, ...,\n",
       "           1.78255141e-03, 1.78255141e-03, 2.67475843e-03],\n",
       "          [1.15875155e-02, 1.15875155e-02, 2.31768936e-02, ...,\n",
       "           8.02241266e-03, 2.31768936e-02, 1.24797225e-02],\n",
       "          ...,\n",
       "          [1.69370323e-02, 2.22846866e-02, 2.85245478e-02, ...,\n",
       "           5.43743372e-02, 8.02241266e-03, 7.13206828e-03],\n",
       "          [1.51544809e-02, 1.96099281e-02, 8.02241266e-03, ...,\n",
       "           4.90266830e-02, 2.13943422e-02, 1.24797225e-02],\n",
       "          [8.91461968e-03, 7.13206828e-03, 4.45730984e-03, ...,\n",
       "           3.83295119e-02, 1.60448253e-02, 9.80496407e-03]],\n",
       "\n",
       "         [[1.69370323e-02, 1.87195837e-02, 1.78273767e-02, ...,\n",
       "           8.02241266e-03, 5.34765422e-03, 8.02241266e-03],\n",
       "          [2.67475843e-03, 4.45730984e-03, 8.02241266e-03, ...,\n",
       "           8.92207026e-04, 0.00000000e+00, 8.92207026e-04],\n",
       "          [5.34765422e-03, 5.34765422e-03, 1.60448253e-02, ...,\n",
       "           2.67475843e-03, 3.20896506e-02, 4.45730984e-03],\n",
       "          ...,\n",
       "          [4.45730984e-03, 2.31768936e-02, 6.41811639e-02, ...,\n",
       "           2.85245478e-02, 6.23986125e-03, 4.45730984e-03],\n",
       "          [1.33719295e-02, 3.11993062e-02, 1.87195837e-02, ...,\n",
       "           2.58497894e-02, 4.10042703e-02, 4.54615802e-02],\n",
       "          [7.13206828e-03, 7.13206828e-03, 2.67475843e-03, ...,\n",
       "           1.33719295e-02, 1.78273767e-02, 3.11993062e-02]],\n",
       "\n",
       "         [[1.17485034e+00, 1.33975780e+00, 9.49330628e-01, ...,\n",
       "           5.14332175e-01, 5.20572066e-01, 4.97395158e-01],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 3.56510282e-03, 0.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 1.15875155e-02, 4.36790287e-02, ...,\n",
       "           1.78255141e-03, 1.78255141e-03, 1.96099281e-02],\n",
       "          [0.00000000e+00, 8.91461968e-03, 8.91461968e-03, ...,\n",
       "           1.69370323e-02, 1.96099281e-02, 2.58497894e-02],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           7.13206828e-03, 7.13206828e-03, 6.23986125e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[2.52263606e-01, 2.85245478e-01, 3.55664641e-01, ...,\n",
       "           5.73163867e-01, 7.31831431e-01, 9.34176147e-01],\n",
       "          [3.17335129e-01, 3.25357556e-01, 4.15388495e-01, ...,\n",
       "           8.98521423e-01, 1.04203451e+00, 1.30053794e+00],\n",
       "          [2.97723353e-01, 4.85807657e-01, 5.04527211e-01, ...,\n",
       "           1.05718708e+00, 1.10888851e+00, 9.20806050e-01],\n",
       "          ...,\n",
       "          [1.78273767e-02, 1.42622739e-02, 6.23986125e-03, ...,\n",
       "           2.22846866e-02, 8.02241266e-03, 8.02241266e-03],\n",
       "          [1.51544809e-02, 1.42622739e-02, 8.91461968e-03, ...,\n",
       "           4.45730984e-03, 8.92207026e-04, 8.92207026e-04],\n",
       "          [8.28988850e-02, 3.03070992e-02, 9.80496407e-03, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 8.92207026e-04]],\n",
       "\n",
       "         [[2.09476799e-01, 2.80788183e-01, 3.09312701e-01, ...,\n",
       "           4.83132899e-01, 4.93830085e-01, 4.78675604e-01],\n",
       "          [2.88810581e-01, 3.28922629e-01, 3.63687038e-01, ...,\n",
       "           8.94954443e-01, 9.38633442e-01, 9.92115617e-01],\n",
       "          [4.05581653e-01, 4.92047518e-01, 3.97559255e-01, ...,\n",
       "           1.08660388e+00, 1.14454329e+00, 1.15969777e+00],\n",
       "          ...,\n",
       "          [6.89044595e-01, 5.50879180e-01, 2.85245478e-01, ...,\n",
       "           1.83627009e-01, 2.94167548e-02, 2.67419964e-02],\n",
       "          [4.05581653e-01, 2.75438666e-01, 1.50645152e-01, ...,\n",
       "           3.74391675e-02, 8.92207026e-04, 8.92207026e-04],\n",
       "          [4.76002693e-01, 2.87918389e-01, 4.10042703e-02, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "         [[2.43349001e-01, 1.46187842e-01, 1.10533088e-01, ...,\n",
       "           3.91319394e-01, 4.17171061e-01, 4.85807657e-01],\n",
       "          [3.30705196e-01, 2.48698518e-01, 2.24629417e-01, ...,\n",
       "           6.67650223e-01, 7.50549138e-01, 8.79801810e-01],\n",
       "          [4.40346062e-01, 4.01126206e-01, 3.98451447e-01, ...,\n",
       "           9.38633442e-01, 1.12047601e+00, 1.18822229e+00],\n",
       "          ...,\n",
       "          [9.37741280e-01, 8.76236677e-01, 6.56062722e-01, ...,\n",
       "           2.96832979e-01, 4.03799117e-01, 1.68472528e-01],\n",
       "          [1.37273967e+00, 1.39413214e+00, 7.96010733e-01, ...,\n",
       "           1.99671835e-01, 3.49424779e-01, 3.36945057e-01],\n",
       "          [1.31123328e+00, 1.00192237e+00, 5.44639289e-01, ...,\n",
       "           3.53882074e-01, 1.14098191e-01, 1.05183572e-01]]],\n",
       "\n",
       "\n",
       "        [[[2.54046172e-01, 2.30869278e-01, 1.46187842e-01, ...,\n",
       "           2.42458656e-01, 2.54046172e-01, 2.05911696e-01],\n",
       "          [3.82406652e-01, 3.46750021e-01, 2.33544037e-01, ...,\n",
       "           4.76893038e-01, 4.84025121e-01, 4.23410892e-01],\n",
       "          [4.66197729e-01, 4.44803387e-01, 3.93103808e-01, ...,\n",
       "           6.72999740e-01, 7.79965878e-01, 8.64647329e-01],\n",
       "          ...,\n",
       "          [1.37541437e+00, 1.35580266e+00, 1.33173537e+00, ...,\n",
       "           8.00468028e-01, 8.15620661e-01, 7.92445600e-01],\n",
       "          [1.57954168e+00, 1.47435808e+00, 1.34867239e+00, ...,\n",
       "           1.13563049e+00, 9.56460834e-01, 6.82804704e-01],\n",
       "          [1.68294275e+00, 1.79525840e+00, 2.06089211e+00, ...,\n",
       "           1.15880561e+00, 7.88880527e-01, 8.63757014e-01]],\n",
       "\n",
       "         [[1.83627009e-01, 1.21228397e-01, 1.30143017e-01, ...,\n",
       "           1.80952251e-01, 2.96832979e-01, 1.48862600e-01],\n",
       "          [3.39619815e-01, 2.67416239e-01, 2.73656100e-01, ...,\n",
       "           3.21792424e-01, 4.22518700e-01, 3.52989882e-01],\n",
       "          [4.92937863e-01, 5.48204422e-01, 5.55336475e-01, ...,\n",
       "           4.72435713e-01, 6.94392264e-01, 7.59463787e-01],\n",
       "          ...,\n",
       "          [1.42889655e+00, 1.44583356e+00, 1.38076210e+00, ...,\n",
       "           1.21407211e+00, 1.07768917e+00, 1.12671590e+00],\n",
       "          [1.69096518e+00, 1.65352786e+00, 1.65887547e+00, ...,\n",
       "           1.20426714e+00, 1.07412410e+00, 1.00192237e+00],\n",
       "          [1.72394705e+00, 1.69007480e+00, 1.70077014e+00, ...,\n",
       "           1.93431413e+00, 2.49678087e+00, 1.37987173e+00]],\n",
       "\n",
       "         [[3.93994153e-01, 5.02744675e-01, 6.07928276e-01, ...,\n",
       "           5.34839928e-02, 3.16442907e-01, 1.60448253e-02],\n",
       "          [2.43349001e-01, 1.92539766e-01, 1.56885013e-01, ...,\n",
       "           1.82734802e-01, 5.25919735e-01, 1.40840188e-01],\n",
       "          [3.86863947e-01, 3.79731894e-01, 2.36218795e-01, ...,\n",
       "           3.71709466e-01, 3.73492002e-01, 4.76893038e-01],\n",
       "          ...,\n",
       "          [1.13652086e+00, 1.22833443e+00, 1.45831335e+00, ...,\n",
       "           1.47525036e+00, 1.36739206e+00, 1.40215647e+00],\n",
       "          [1.82467330e+00, 1.95570850e+00, 1.80506337e+00, ...,\n",
       "           1.73553455e+00, 1.66600752e+00, 1.54121220e+00],\n",
       "          [2.08941650e+00, 1.94144619e+00, 1.90757406e+00, ...,\n",
       "           1.80060601e+00, 1.74623168e+00, 2.05465221e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.01083517e+00, 1.18376505e+00, 1.10978079e+00, ...,\n",
       "           2.08050203e+00, 2.08585143e+00, 1.63480830e+00],\n",
       "          [1.54388690e+00, 1.55101717e+00, 1.39145923e+00, ...,\n",
       "           2.15627074e+00, 2.56541920e+00, 2.28552151e+00],\n",
       "          [1.67046297e+00, 1.89152920e+00, 1.43424606e+00, ...,\n",
       "           1.18733013e+00, 1.85319781e+00, 2.18479514e+00],\n",
       "          ...,\n",
       "          [1.59559771e-01, 2.47806311e-01, 3.34270298e-01, ...,\n",
       "           2.25521624e-01, 2.23739073e-01, 9.71611589e-02],\n",
       "          [8.82484019e-02, 1.69364735e-01, 3.14660370e-01, ...,\n",
       "           7.93337822e-02, 3.74391675e-02, 8.02241266e-03],\n",
       "          [6.59637153e-02, 1.06966123e-01, 1.87192112e-01, ...,\n",
       "           9.00309533e-02, 3.56510282e-03, 2.67475843e-03]],\n",
       "\n",
       "         [[1.67046297e+00, 1.53408194e+00, 1.25329387e+00, ...,\n",
       "           1.81754303e+00, 2.52887225e+00, 2.26947665e+00],\n",
       "          [1.45920372e+00, 1.23189950e+00, 1.19267964e+00, ...,\n",
       "           1.31836534e+00, 2.26323676e+00, 2.30067587e+00],\n",
       "          [7.65703619e-01, 1.32817030e+00, 1.54477727e+00, ...,\n",
       "           1.32193041e+00, 1.29073119e+00, 2.18390489e+00],\n",
       "          ...,\n",
       "          [4.90266830e-02, 9.71611589e-02, 2.16607004e-01, ...,\n",
       "           5.74054182e-01, 5.68706512e-01, 7.12219596e-01],\n",
       "          [2.13943422e-02, 5.25917858e-02, 8.37910920e-02, ...,\n",
       "           2.95940787e-01, 2.70090997e-01, 2.38001347e-01],\n",
       "          [1.15875155e-02, 2.40672380e-02, 4.27868217e-02, ...,\n",
       "           1.60450116e-01, 1.73822045e-01, 6.59637153e-02]],\n",
       "\n",
       "         [[1.83448005e+00, 2.06623983e+00, 2.17320776e+00, ...,\n",
       "           2.35326767e+00, 2.70536733e+00, 2.34792018e+00],\n",
       "          [1.23635674e+00, 1.60093606e+00, 1.88885438e+00, ...,\n",
       "           2.14824820e+00, 2.35505033e+00, 2.16875029e+00],\n",
       "          [4.16278839e-01, 6.41800463e-01, 1.21852946e+00, ...,\n",
       "           2.00562549e+00, 1.48148835e+00, 2.23560452e+00],\n",
       "          ...,\n",
       "          [1.91649422e-01, 2.07694247e-01, 1.47080049e-01, ...,\n",
       "           5.88316441e-01, 8.99411738e-01, 9.67158020e-01],\n",
       "          [3.11993062e-02, 2.85245478e-02, 3.03070992e-02, ...,\n",
       "           1.47080049e-01, 3.39619815e-01, 3.89536858e-01],\n",
       "          [1.78255141e-03, 1.78255141e-03, 2.67475843e-03, ...,\n",
       "           9.35960561e-02, 1.17663294e-01, 1.38165429e-01]]],\n",
       "\n",
       "\n",
       "        [[[2.04930449e+00, 2.26234651e+00, 2.29532838e+00, ...,\n",
       "           2.37822723e+00, 2.64118624e+00, 2.53243732e+00],\n",
       "          [1.42265666e+00, 1.72305477e+00, 1.82645774e+00, ...,\n",
       "           2.32385278e+00, 2.60642171e+00, 2.63494635e+00],\n",
       "          [3.36054713e-01, 3.81514430e-01, 7.79965878e-01, ...,\n",
       "           1.95036077e+00, 2.27125931e+00, 2.86314273e+00],\n",
       "          ...,\n",
       "          [1.89866871e-01, 2.61176378e-01, 2.24629417e-01, ...,\n",
       "           4.44803387e-01, 4.76002693e-01, 8.55734587e-01],\n",
       "          [2.49594450e-02, 4.72441316e-02, 4.90266830e-02, ...,\n",
       "           1.95214525e-01, 2.45131552e-01, 4.15388495e-01],\n",
       "          [1.78255141e-03, 1.78255141e-03, 2.67475843e-03, ...,\n",
       "           9.80533659e-02, 9.44882631e-02, 1.29250810e-01]],\n",
       "\n",
       "         [[1.85587263e+00, 2.21421194e+00, 2.47895336e+00, ...,\n",
       "           2.80074596e+00, 2.51282740e+00, 2.36039972e+00],\n",
       "          [1.80595553e+00, 2.08585143e+00, 2.25432396e+00, ...,\n",
       "           2.98793793e+00, 2.82124805e+00, 2.74191427e+00],\n",
       "          [1.07412410e+00, 1.05183947e+00, 1.06788421e+00, ...,\n",
       "           1.92183626e+00, 2.23916960e+00, 2.50569534e+00],\n",
       "          ...,\n",
       "          [3.12877834e-01, 2.05911696e-01, 1.68472528e-01, ...,\n",
       "           2.55828738e-01, 3.02180648e-01, 4.43911195e-01],\n",
       "          [9.62708145e-02, 4.99188900e-02, 7.57686794e-02, ...,\n",
       "           1.62232667e-01, 2.01454386e-01, 2.34436244e-01],\n",
       "          [6.23986125e-03, 6.23986125e-03, 1.60448253e-02, ...,\n",
       "           1.27468258e-01, 7.48764724e-02, 1.51535496e-01]],\n",
       "\n",
       "         [[1.34599769e+00, 1.74890649e+00, 2.29176140e+00, ...,\n",
       "           2.73478222e+00, 2.61266160e+00, 2.25788903e+00],\n",
       "          [1.51268768e+00, 1.73286164e+00, 2.32206845e+00, ...,\n",
       "           3.16532326e+00, 3.16354084e+00, 2.95406389e+00],\n",
       "          [1.89420211e+00, 1.67224741e+00, 1.78188825e+00, ...,\n",
       "           2.31493831e+00, 2.67149329e+00, 2.97367573e+00],\n",
       "          ...,\n",
       "          [3.92211616e-01, 3.11095268e-01, 2.56720930e-01, ...,\n",
       "           2.83462942e-01, 3.77949327e-01, 4.82242554e-01],\n",
       "          [7.77291119e-01, 8.50385070e-01, 2.01454386e-01, ...,\n",
       "           2.07694247e-01, 2.55828738e-01, 2.95940787e-01],\n",
       "          [3.57447207e-01, 3.26247871e-01, 7.84415752e-02, ...,\n",
       "           2.21064314e-01, 1.41730532e-01, 1.64907426e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[6.81022167e-01, 4.92937863e-01, 3.40510160e-01, ...,\n",
       "           3.92217189e-02, 6.23967499e-02, 1.24797225e-02],\n",
       "          [7.58573413e-01, 6.19515777e-01, 4.90264952e-01, ...,\n",
       "           9.89437103e-02, 1.18555501e-01, 4.72441316e-02],\n",
       "          [7.08654523e-01, 6.41800463e-01, 5.90099037e-01, ...,\n",
       "           2.53153980e-01, 1.45297498e-01, 8.46814364e-02],\n",
       "          ...,\n",
       "          [3.69926929e-01, 4.32323664e-01, 4.92937863e-01, ...,\n",
       "           4.75110471e-01, 4.66197729e-01, 3.40510160e-01],\n",
       "          [4.17171061e-01, 3.11095268e-01, 3.93994153e-01, ...,\n",
       "           4.03799117e-01, 3.96668911e-01, 3.54772449e-01],\n",
       "          [4.66197729e-01, 2.55828738e-01, 2.95050442e-01, ...,\n",
       "           4.10038978e-01, 4.09148633e-01, 4.10931170e-01]],\n",
       "\n",
       "         [[1.02866256e+00, 9.08326387e-01, 7.16676950e-01, ...,\n",
       "           9.71611589e-02, 0.00000000e+00, 0.00000000e+00],\n",
       "          [1.18643975e+00, 1.07323384e+00, 8.66429865e-01, ...,\n",
       "           1.96106732e-01, 3.29818577e-02, 3.65469605e-02],\n",
       "          [1.07858145e+00, 1.12404299e+00, 9.68048334e-01, ...,\n",
       "           3.66361797e-01, 1.50645152e-01, 1.14098191e-01],\n",
       "          ...,\n",
       "          [7.30048895e-01, 6.38235331e-01, 7.11329281e-01, ...,\n",
       "           6.36452794e-01, 6.08818591e-01, 4.70653176e-01],\n",
       "          [8.77127051e-01, 7.92445600e-01, 7.06871986e-01, ...,\n",
       "           5.82968831e-01, 5.58009386e-01, 5.27702272e-01],\n",
       "          [9.43090796e-01, 9.60918128e-01, 6.81912541e-01, ...,\n",
       "           4.58173454e-01, 4.62630749e-01, 5.58009386e-01]],\n",
       "\n",
       "         [[1.18822229e+00, 9.38633442e-01, 8.20970178e-01, ...,\n",
       "           1.96997076e-01, 3.07530165e-01, 3.02180648e-01],\n",
       "          [1.26309884e+00, 9.90333021e-01, 8.18295419e-01, ...,\n",
       "           3.77057135e-01, 4.98287380e-01, 4.04691339e-01],\n",
       "          [1.39324176e+00, 9.68940556e-01, 8.51277232e-01, ...,\n",
       "           6.54280186e-01, 3.93994153e-01, 3.77949327e-01],\n",
       "          ...,\n",
       "          [1.28894854e+00, 9.86767948e-01, 7.53223896e-01, ...,\n",
       "           1.23813939e+00, 1.10978079e+00, 9.44873333e-01],\n",
       "          [1.09997392e+00, 7.53223896e-01, 6.54280186e-01, ...,\n",
       "           1.19089711e+00, 1.18109024e+00, 1.03222954e+00],\n",
       "          [1.16772020e+00, 6.03470922e-01, 6.68542445e-01, ...,\n",
       "           9.35958683e-01, 1.13563049e+00, 1.08036399e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.00000000e+00, 7.00727105e-03, 7.00727105e-03, ...,\n",
       "           2.40281224e-02, 3.30358744e-02, 1.40145421e-02],\n",
       "          [9.01147723e-03, 1.50166452e-02, 1.00135803e-02, ...,\n",
       "           4.00431454e-02, 5.20572066e-02, 2.80290842e-02],\n",
       "          [1.20140612e-02, 1.80192292e-02, 2.00420618e-03, ...,\n",
       "           2.30260193e-02, 2.90311873e-02, 2.10218132e-02],\n",
       "          ...,\n",
       "          [2.00420618e-03, 5.00679016e-03, 9.01147723e-03, ...,\n",
       "           2.00420618e-03, 6.00889325e-03, 8.00937414e-03],\n",
       "          [4.00468707e-03, 7.00727105e-03, 6.00889325e-03, ...,\n",
       "           5.60581684e-02, 8.50893557e-02, 7.70837069e-02],\n",
       "          [5.00679016e-03, 2.00420618e-03, 1.00210309e-03, ...,\n",
       "           2.36246735e-01, 9.50992107e-02, 3.00332904e-02]],\n",
       "\n",
       "         [[0.00000000e+00, 1.00210309e-03, 6.00889325e-03, ...,\n",
       "           1.40145421e-02, 7.00727105e-03, 2.20239162e-02],\n",
       "          [0.00000000e+00, 1.00210309e-03, 1.00135803e-02, ...,\n",
       "           3.40379775e-02, 2.70307064e-02, 4.10452485e-02],\n",
       "          [0.00000000e+00, 0.00000000e+00, 8.00937414e-03, ...,\n",
       "           3.00332904e-02, 3.60384583e-02, 1.70208514e-02],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           7.00727105e-03, 3.80426645e-02, 2.90311873e-02],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           1.70208514e-02, 3.80426645e-02, 6.10649586e-02],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           5.10551035e-02, 1.42149627e-01, 2.30260193e-02]],\n",
       "\n",
       "         [[2.00420618e-03, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           1.00210309e-03, 5.00679016e-03, 1.80192292e-02],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           1.00135803e-02, 1.70208514e-02, 3.30358744e-02],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           1.60187483e-02, 2.20239162e-02, 2.00234354e-02],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 1.40145421e-02],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           1.00210309e-03, 0.00000000e+00, 2.80290842e-02],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           3.00630927e-03, 7.70837069e-02, 1.36144459e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.11117959e-01, 1.24130398e-01, 6.40675426e-02, ...,\n",
       "           4.80525196e-02, 1.30161643e-02, 1.00210309e-03],\n",
       "          [1.02106482e-01, 8.50893557e-02, 4.70504165e-02, ...,\n",
       "           7.50795007e-02, 3.80426645e-02, 2.10218132e-02],\n",
       "          [7.60816038e-02, 9.81055200e-02, 6.00628555e-02, ...,\n",
       "           1.60187483e-02, 1.70178711e-01, 1.33141875e-01],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [2.00234354e-02, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "         [[4.80525196e-02, 4.80525196e-02, 4.30457294e-02, ...,\n",
       "           1.56164169e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "          [6.20670617e-02, 5.70602715e-02, 4.80525196e-02, ...,\n",
       "           2.15224922e-01, 1.10119581e-02, 7.00727105e-03],\n",
       "          [1.96207315e-01, 5.80623746e-02, 5.10551035e-02, ...,\n",
       "           3.60384583e-02, 4.40478325e-02, 1.00135803e-02],\n",
       "          ...,\n",
       "          [6.80722296e-02, 1.36144459e-01, 1.05112791e-01, ...,\n",
       "           2.00420618e-03, 2.00420618e-03, 2.00420618e-03],\n",
       "          [5.00530005e-02, 5.70602715e-02, 6.90743327e-02, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [1.11117959e-01, 3.30358744e-02, 1.70208514e-02, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "         [[0.00000000e+00, 7.40773976e-02, 4.50499356e-02, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [2.28241086e-01, 2.47258693e-01, 5.80623746e-02, ...,\n",
       "           1.10119581e-02, 1.40145421e-02, 9.01147723e-03],\n",
       "          [3.04318964e-01, 4.56478447e-01, 1.60187483e-02, ...,\n",
       "           1.70208514e-02, 3.60384583e-02, 1.00135803e-02],\n",
       "          ...,\n",
       "          [0.00000000e+00, 1.40145421e-02, 5.40576875e-02, ...,\n",
       "           5.30555844e-02, 2.04212964e-01, 4.90508974e-02],\n",
       "          [2.20239162e-02, 4.70504165e-02, 9.61013138e-02, ...,\n",
       "           1.00210309e-03, 3.70405614e-02, 0.00000000e+00],\n",
       "          [8.70935619e-02, 1.65171921e-01, 1.34140253e-01, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],\n",
       "\n",
       "\n",
       "        [[[0.00000000e+00, 1.00210309e-03, 4.00468707e-03, ...,\n",
       "           2.07219273e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 4.70504165e-02, 1.21127814e-01, ...,\n",
       "           1.20140612e-02, 1.70208514e-02, 1.00135803e-02],\n",
       "          [2.70307064e-02, 1.88197941e-01, 4.28449363e-01, ...,\n",
       "           2.20239162e-02, 3.30358744e-02, 1.10119581e-02],\n",
       "          ...,\n",
       "          [0.00000000e+00, 4.00468707e-03, 1.90213323e-02, ...,\n",
       "           7.70837069e-02, 3.83399427e-01, 3.02314758e-01],\n",
       "          [0.00000000e+00, 4.00468707e-03, 1.70208514e-02, ...,\n",
       "           4.30457294e-02, 7.30790198e-02, 5.40576875e-02],\n",
       "          [1.70208514e-02, 6.20670617e-02, 9.10982490e-02, ...,\n",
       "           3.00630927e-03, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "         [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           3.76392156e-01, 4.70504165e-02, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           3.13326716e-01, 7.20769167e-02, 7.00727105e-03],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           1.00135803e-02, 2.40281224e-02, 9.01147723e-03],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           1.50166452e-02, 3.00630927e-03, 7.00727105e-03],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           1.03108585e-01, 2.01210380e-01, 1.62169337e-01],\n",
       "          [0.00000000e+00, 8.00937414e-03, 2.10218132e-02, ...,\n",
       "           1.28135085e-01, 7.10748136e-02, 6.80722296e-02]],\n",
       "\n",
       "         [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           3.67384404e-01, 7.00727105e-03, 3.00630927e-03],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           3.80396843e-01, 1.70208514e-02, 1.50166452e-02],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           2.65277922e-01, 2.60286033e-02, 2.40281224e-02],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           2.10218132e-02, 4.60483134e-02, 2.10218132e-02],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           6.50696456e-02, 2.00234354e-02, 6.00889325e-03],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           5.40576875e-02, 2.50265002e-02, 2.20239162e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "         [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "         [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],\n",
       "\n",
       "\n",
       "        [[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "         [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "         [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "         [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          ...,\n",
       "          [1.00210309e-03, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
       "\n",
       "         [[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [1.00210309e-03, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          ...,\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "          [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "           0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]]],\n",
       "       dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a7602-b6a4-4b6e-ae9c-da587785091c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89612223-1fe0-476a-93d5-c396e58e3853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "finished loading data\n",
      "building networks\n",
      "finished building networks\n",
      "start training on 3 samples\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "  0%|          | 0/50 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'j' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-32b6686cd2e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mn_epoch_and_batch_size_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m     \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;31m#this is only needed for correct plot labelling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-32b6686cd2e2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, _batch_size, start_epoch)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch {epoch:04d}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{plotdirs}/fake_samples_{epoch:04d}_{j:06d}.{plot_format}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#cambio i por j\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# plot loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'j' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('agg')\n",
    "from pylab import plt\n",
    "from tqdm import trange\n",
    "from skimage.util import view_as_windows\n",
    "from matplotlib.colors import LogNorm\n",
    "from tensorflow.keras.utils import GeneratorEnqueuer\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "#PARAMS\n",
    "# for training data:\n",
    "startdate = '19700101'\n",
    "enddate = '19741231'\n",
    "\n",
    "ndomain = 16  # gridpoints\n",
    "stride = 16\n",
    "\n",
    "tp_thresh_daily = 5  # mm. in the radardate the unit is mm/h, but then on 5 minutes steps.\n",
    "# the conversion is done automatically in this script\n",
    "n_thresh = 20\n",
    "\n",
    "# normalization of daily sums\n",
    "# we ues the 99.9 percentile of 2010\n",
    "#norm_scale = 127.4\n",
    "norm_scale = 1\n",
    "\n",
    "# neural network parameters\n",
    "n_disc = 5\n",
    "GRADIENT_PENALTY_WEIGHT = 10  # As per the paper\n",
    "latent_dim = 100\n",
    "batch_size = 32 # this is used as global variable in randomweightedaverage\n",
    "# the training is done with increasing batch size. each tuple is\n",
    "# a combination nof number of epochs and batch_size\n",
    "#n_epoch_and_batch_size_list = ((5, 32), (10, 64), (10, 128), (20, 256))\n",
    "n_epoch_and_batch_size_list = ((50, 32),)\n",
    "\n",
    "plot_format = 'png'\n",
    "\n",
    "name='results'\n",
    "\n",
    "plotdirs = f'/home/jovyan/work/plots_{name}/'\n",
    "\n",
    "outdirs = f'/home/jovyan/work/trained_models/{name}/'\n",
    "\n",
    "# note for colab: sometimes mkdir does not work that way. in this case\n",
    "# you have to create the directories manually\n",
    "os.system(f'mkdir -p {plotdirs}')\n",
    "os.system(f'mkdir -p {outdirs}')\n",
    "\n",
    "# load data and precomputed indices\n",
    "\n",
    "converted_data_paths = '/home/jovyan/work/prueba_datos_ERA5/'\n",
    "\n",
    "indices_data_paths = '/home/jovyan/work/prueba_datos_ERA5/'\n",
    "\n",
    "data_ifile = f'{converted_data_paths}/prueba{startdate}-{enddate}.npy'\n",
    "\n",
    "indices_file = f'{indices_data_paths}/valid_indices_ERA5-{startdate}-{enddate}.pkl'\n",
    "print('loading data')\n",
    "# load the data as memmap\n",
    "data = np.load(data_ifile, mmap_mode='r')\n",
    "\n",
    "\n",
    "indices_all = pickle.load(open(indices_file, 'rb'))\n",
    "# convert to array\n",
    "indices_all = np.array(indices_all)\n",
    "# this has shape (nsamples,3)\n",
    "# each row is (tidx,yidx,xidx)\n",
    "print('finished loading data')\n",
    "\n",
    "# the data has dimensions (sample,hourofday,x,y)\n",
    "n_days, nhours, ny, nx = data.shape\n",
    "n_channel=1\n",
    "# sanity checks\n",
    "assert (len(data.shape) == 4)\n",
    "assert (len(indices_all.shape) == 2)\n",
    "assert (indices_all.shape[1] == 3)\n",
    "assert (nhours == 24 // 1)\n",
    "assert (np.max(indices_all[:, 0]) < n_days)\n",
    "assert (np.max(indices_all[:, 1]) < ny)\n",
    "assert (np.max(indices_all[:, 2]) < nx)\n",
    "assert (data.dtype == 'float32')\n",
    "\n",
    "n_samples = len(indices_all)\n",
    "\n",
    "\n",
    "def generate_real_samples(n_batch):\n",
    "    \"\"\"get random sampples and do the last preprocessing on them\"\"\"\n",
    "    while True:\n",
    "        # get random sample of indices from the precomputed indices\n",
    "        # for this we generate random indices for the index list (confusing termoonology, since we use\n",
    "        # indices to index the list of indices...\n",
    "        ixs = np.random.randint(n_samples, size=n_batch)\n",
    "        idcs_batch = indices_all[ixs]\n",
    "\n",
    "        # now we select the data corresponding to these indices\n",
    "\n",
    "        data_wview = view_as_windows(data, (1, 1, ndomain, ndomain))[..., 0, 0, :,:]\n",
    "        batch = data_wview[idcs_batch[:, 0], :, idcs_batch[:, 1], idcs_batch[:, 2]]\n",
    "        # add empty channel dimension (necessary for keras, which expects a channel dimension)\n",
    "        batch = np.expand_dims(batch, -1)\n",
    "        # compute daily sum (which is the condition)\n",
    "        batch_cond = np.sum(batch, axis=1) # daily sum\n",
    "\n",
    "        # the data now is in mm/hour, but we want it as fractions of the daily sum for each day\n",
    "        for i in range(n_batch):\n",
    "            batch[i] = batch[i] / batch_cond[i]\n",
    "\n",
    "        # normalize daily sum\n",
    "        batch_cond = batch_cond / norm_scale\n",
    "        assert (batch.shape == (n_batch, nhours, ndomain, ndomain, 1))\n",
    "        assert (batch_cond.shape == (n_batch, ndomain, ndomain, 1))\n",
    "        assert (~np.any(np.isnan(batch)))\n",
    "        assert (~np.any(np.isnan(batch_cond)))\n",
    "        assert (np.max(batch) <= 1)\n",
    "        assert (np.min(batch) >= 0)\n",
    "\n",
    "        yield [batch, batch_cond]\n",
    "\n",
    "\n",
    "def generate_latent_points(n_batch):\n",
    "    # generate points in the latent space and a random condition\n",
    "    latent = np.random.normal(size=(n_batch, latent_dim))\n",
    "    # randomly select conditions\n",
    "    ixs = np.random.randint(0, n_samples, size=n_batch)\n",
    "    idcs_batch = indices_all[ixs]\n",
    "\n",
    "    data_wview = view_as_windows(data, (1, 1, ndomain, ndomain))[..., 0, 0, :,:]\n",
    "    batch = data_wview[idcs_batch[:, 0], :, idcs_batch[:, 1], idcs_batch[:, 2]]\n",
    "    # add empty channel dimension (necessary for keras, which expects a channel dimension)\n",
    "    batch = np.expand_dims(batch, -1)\n",
    "    batch_cond = np.sum(batch, axis=1) # daily sum\n",
    "    # normalize daily sum\n",
    "    batch_cond = batch_cond / norm_scale\n",
    "    assert (batch_cond.shape == (n_batch, ndomain, ndomain, 1))\n",
    "    assert (~np.any(np.isnan(batch_cond)))\n",
    "    return [latent, batch_cond]\n",
    "\n",
    "\n",
    "def generate_latent_points_as_generator(n_batch):\n",
    "    while True:\n",
    "        yield generate_latent_points(n_batch)\n",
    "\n",
    "\n",
    "def generate_fake_samples(n_batch):\n",
    "    # generate points in latent space\n",
    "    latent, cond = generate_latent_points(n_batch)\n",
    "    # predict outputs\n",
    "    generated = generator.predict([latent, cond])\n",
    "    return [generated, cond]\n",
    "\n",
    "\n",
    "def generate(cond):\n",
    "    latent = np.random.normal(size=(1, latent_dim))\n",
    "    cond = np.expand_dims(cond, 0)\n",
    "    return generator.predict([latent, cond])\n",
    "\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "class RandomWeightedAverage(tf.keras.layers.Layer):\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        global batch_size\n",
    "        alpha = tf.random.uniform((batch_size,1, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "\n",
    "\n",
    "class GradientPenalty(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GradientPenalty, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        super(GradientPenalty, self).build(input_shapes)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, inputs):\n",
    "        target, wrt = inputs\n",
    "        grad = K.gradients(target, wrt)[0]\n",
    "        return K.sqrt(K.sum(K.batch_flatten(K.square(grad)), axis=1, keepdims=True))-1\n",
    "\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        return (input_shapes[1][0], 1)\n",
    "\n",
    "\n",
    "# pixel-wise feature vector normalization layer\n",
    "# from https://machinelearningmastery.com/how-to-train-a-progressive-growing-gan-in-keras-for-synthesizing-faces/\n",
    "class PixelNormalization(tf.keras.layers.Layer):\n",
    "    # initialize the layer\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PixelNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    # perform the operation\n",
    "    def call(self, inputs):\n",
    "        # calculate square pixel values\n",
    "        values = inputs ** 2.0\n",
    "        # calculate the mean pixel values\n",
    "        mean_values = K.mean(values, axis=-1, keepdims=True)\n",
    "        # ensure the mean is not zero\n",
    "        mean_values += 1.0e-8\n",
    "        # calculate the sqrt of the mean squared value (L2 norm)\n",
    "        l2 = K.sqrt(mean_values)\n",
    "        # normalize values by the l2 norm\n",
    "        normalized = inputs / l2\n",
    "        return normalized\n",
    "\n",
    "    # define the output shape of the layer\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def create_discriminator():\n",
    "    # we add the condition as additional channel. For this we\n",
    "    # expand its dimensions alon the nhours axis via linear scaling\n",
    "    in_cond = tf.keras.layers.Input(shape=(ndomain, ndomain, 1))\n",
    "    # add nhours dimension (size 1 for now)\n",
    "    cond_expanded = tf.keras.layers.Reshape((1, ndomain, ndomain, 1))(in_cond)\n",
    "    cond_expanded = tf.keras.layers.Lambda(lambda x: tf.keras.backend.repeat_elements(x, rep=nhours, axis=1))(\n",
    "        cond_expanded)\n",
    "    in_sample = tf.keras.layers.Input(shape=(nhours, ndomain, ndomain, 1))\n",
    "\n",
    "    in_combined = tf.keras.layers.Concatenate(axis=-1)([in_sample, cond_expanded])\n",
    "    kernel_size = (3, 3, 3)\n",
    "    main_net = tf.keras.Sequential([\n",
    "\n",
    "        tf.keras.layers.Conv3D(64, kernel_size=kernel_size, strides=2, input_shape=(nhours, ndomain, ndomain, 2),\n",
    "                               padding=\"valid\"),  # 11x7x7x32\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv3D(128, kernel_size=kernel_size, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv3D(256, kernel_size=kernel_size, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv3D(256, kernel_size=kernel_size, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1, activation='linear'),\n",
    "    ])\n",
    "    out = main_net(in_combined)\n",
    "    model = tf.keras.Model(inputs=[in_sample, in_cond], outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_generator():\n",
    "\n",
    "    # for the moment, the flat approach is used\n",
    "    init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    # define model\n",
    "\n",
    "    n_nodes = 256 * 2 * 2 * 3\n",
    "    in_latent = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "    # the condition is a 2d array (ndomain x ndomain), we simply flatten it\n",
    "    in_cond = tf.keras.layers.Input(shape=(ndomain, ndomain, n_channel))\n",
    "    in_cond_flat = tf.keras.layers.Flatten()(in_cond)\n",
    "    in_combined = tf.keras.layers.Concatenate()([in_latent, in_cond_flat])\n",
    "\n",
    "    main_net = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(n_nodes, kernel_initializer=init),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Reshape((3, 2, 2, 256)),\n",
    "\n",
    "        tf.keras.layers.UpSampling3D(size=(2, 2, 2)),\n",
    "        tf.keras.layers.Conv3D(256, (3, 3, 3), padding='same', kernel_initializer=init),\n",
    "        PixelNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "        tf.keras.layers.UpSampling3D(size=(2, 2, 2)),\n",
    "        tf.keras.layers.Conv3D(128, (3, 3, 3), padding='same', kernel_initializer=init),\n",
    "        PixelNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "        tf.keras.layers.UpSampling3D(size=(2, 2, 2)),\n",
    "        tf.keras.layers.Conv3D(64, (3, 3, 3), padding='same', kernel_initializer=init),\n",
    "        PixelNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        # output 24x16x16x1\n",
    "        tf.keras.layers.Conv3D(1, (3, 3, 3), activation='linear', padding='same', kernel_initializer=init),\n",
    "        # softmax per gridpoint, thus over nhours, which is axis 1 (Softmax also counts the batch axis)\n",
    "        tf.keras.layers.Softmax(axis=1),\n",
    "        # check for Nans (only for debugging)\n",
    "        tf.keras.layers.Lambda(\n",
    "            lambda x: tf.debugging.check_numerics(x, 'found nan in output of per_gridpoint_softmax')),\n",
    "\n",
    "    ])\n",
    "\n",
    "    out = main_net(in_combined)\n",
    "    model = tf.keras.Model(inputs=[in_latent, in_cond], outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print('building networks')\n",
    "generator = create_generator()\n",
    "critic = create_discriminator()\n",
    "generator.trainable = False\n",
    "# Image input (real sample)\n",
    "real_img = tf.keras.layers.Input(shape=(nhours,ndomain,ndomain,n_channel))\n",
    "# Noise input\n",
    "z_disc = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "# Generate image based of noise (fake sample) and add label to the input\n",
    "label = tf.keras.layers.Input(shape=(ndomain, ndomain, n_channel))\n",
    "fake_img = generator([z_disc, label])\n",
    "# Discriminator determines validity of the real and fake images\n",
    "fake = critic([fake_img, label])\n",
    "valid = critic([real_img, label])\n",
    "\n",
    "# Construct weighted average between real and fake images\n",
    "interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "\n",
    "# Determine validity of weighted sample\n",
    "validity_interpolated = critic([interpolated_img, label])\n",
    "# here we use the approach from https://github.com/jleinonen/geogan/blob/master/geogan/gan.py,\n",
    "# where gradient panely is a keras layer, and then 'mse' used as loss for this output\n",
    "disc_gp = GradientPenalty()([validity_interpolated, interpolated_img])\n",
    "\n",
    "# default from https://arxiv.org/pdf/1704.00028.pdf\n",
    "optimizer = tf.optimizers.Adam(lr=0.0001, beta_1=0, beta_2=0.9)\n",
    "\n",
    "critic_model = tf.keras.Model(inputs=[real_img, label, z_disc], outputs=[valid, fake, disc_gp])\n",
    "critic_model.compile(loss=[wasserstein_loss,\n",
    "                                wasserstein_loss,\n",
    "                                'mse'],\n",
    "                          optimizer=optimizer,\n",
    "                          loss_weights=[1, 1, 10])\n",
    "\n",
    "# For the generator we freeze the critic's layers\n",
    "critic.trainable = False\n",
    "generator.trainable = True\n",
    "\n",
    "# Sampled noise for input to generator\n",
    "z_gen = Input(shape=(latent_dim,))\n",
    "# add label to the input\n",
    "label = tf.keras.layers.Input(shape=(ndomain, ndomain, n_channel))\n",
    "# Generate images based of noise\n",
    "img = generator([z_gen, label])\n",
    "# Discriminator determines validity\n",
    "valid = critic([img, label])\n",
    "# Defines generator model\n",
    "generator_model = tf.keras.Model([z_gen, label], valid)\n",
    "generator_model.compile(loss=wasserstein_loss, optimizer=optimizer)\n",
    "print('finished building networks')\n",
    "\n",
    "# plot some real samples\n",
    "# plot a couple of samples\n",
    "plt.figure(figsize=(25, 25))\n",
    "n_plot = 30\n",
    "[X_real, cond_real] = next(generate_real_samples(n_plot))\n",
    "for i in range(n_plot):\n",
    "    plt.subplot(n_plot, 25, i * 25 + 1)\n",
    "    plt.imshow(cond_real[i, :, :].squeeze(), cmap=plt.cm.gist_earth_r, norm=LogNorm(vmin=0.01, vmax=1))\n",
    "    plt.axis('off')\n",
    "    for j in range(1, 24):\n",
    "        plt.subplot(n_plot, 25, i * 25 + j + 1)\n",
    "        plt.imshow(X_real[i, j, :, :].squeeze(), vmin=0, vmax=1, cmap=plt.cm.hot_r)\n",
    "        plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.savefig(f'{plotdirs}/real_samples.{plot_format}')\n",
    "\n",
    "hist = {'d_loss': [], 'g_loss': []}\n",
    "print(f'start training on {n_samples} samples')\n",
    "\n",
    "\n",
    "def train(n_epochs, _batch_size, start_epoch=0):\n",
    "    \"\"\"\n",
    "        train with fixed batch_size for given epochs\n",
    "        make some example plots and save model after each epoch\n",
    "    \"\"\"\n",
    "    global batch_size\n",
    "    batch_size = _batch_size\n",
    "    # create a dataqueue with the keras facilities. this allows\n",
    "    # to prepare the data in parallel to the training\n",
    "    sample_dataqueue = GeneratorEnqueuer(generate_real_samples(batch_size),\n",
    "                                         use_multiprocessing=True)\n",
    "    sample_dataqueue.start(workers=2, max_queue_size=10)\n",
    "    sample_gen = sample_dataqueue.get()\n",
    "\n",
    "    # targets for loss function\n",
    "    gan_sample_dataqueue = GeneratorEnqueuer(generate_latent_points_as_generator(batch_size),\n",
    "                                         use_multiprocessing=True)\n",
    "    gan_sample_dataqueue.start(workers=2, max_queue_size=10)\n",
    "    gan_sample_gen = gan_sample_dataqueue.get()\n",
    "\n",
    "    # targets for loss function\n",
    "    valid = -np.ones((batch_size, 1))\n",
    "    fake = np.ones((batch_size, 1))\n",
    "    dummy = np.zeros((batch_size, 1))  # Dummy gt for gradient penalty\n",
    "\n",
    "    bat_per_epo = int(n_samples / batch_size)\n",
    "\n",
    "    # we need to call the discriminator once in order\n",
    "    # to initialize the input shapes\n",
    "    [X_real, cond_real] = next(sample_gen)\n",
    "    latent = np.random.normal(size=(batch_size, latent_dim))\n",
    "    critic_model.predict([X_real, cond_real, latent])\n",
    "    for i in trange(n_epochs):\n",
    "        epoch = 1 + i + start_epoch\n",
    "        # enumerate batches over the training set\n",
    "        for j in trange(bat_per_epo):\n",
    "\n",
    "            for _ in range(n_disc):\n",
    "                # fetch a batch from the queue\n",
    "                [X_real, cond_real] = next(sample_gen)\n",
    "                latent = np.random.normal(size=(batch_size, latent_dim))\n",
    "                d_loss = critic_model.train_on_batch([X_real, cond_real,latent], [valid, fake, dummy])\n",
    "                # we get for losses back here. average, valid, fake, and gradient_penalty\n",
    "                # we want the average of valid and fake\n",
    "                d_loss = np.mean([d_loss[1], d_loss[2]])\n",
    "\n",
    "\n",
    "            # train generator\n",
    "            # prepare points in latent space as input for the generator\n",
    "            [latent, cond] = next(gan_sample_gen)\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = generator_model.train_on_batch([latent, cond], valid)\n",
    "            # summarize loss on this batch\n",
    "            print(f'{epoch}, {j + 1}/{bat_per_epo}, d_loss {d_loss}' + \\\n",
    "                  f' g:{g_loss} ')  # , d_fake:{d_loss_fake} d_real:{d_loss_real}')\n",
    "\n",
    "            if np.isnan(g_loss) or np.isnan(d_loss):\n",
    "                raise ValueError('encountered nan in g_loss and/or d_loss')\n",
    "\n",
    "            hist['d_loss'].append(d_loss)\n",
    "            hist['g_loss'].append(g_loss)\n",
    "\n",
    "\n",
    "        # plot generated examples\n",
    "        plt.figure(figsize=(25, 25))\n",
    "        n_plot = 30\n",
    "        X_fake, cond_fake = generate_fake_samples(n_plot)\n",
    "        for iplot in range(n_plot):\n",
    "            plt.subplot(n_plot, 25, iplot * 25 + 1)\n",
    "            plt.imshow(cond_fake[iplot, :, :].squeeze(), cmap=plt.cm.gist_earth_r, norm=LogNorm(vmin=0.01, vmax=1))\n",
    "            plt.axis('off')\n",
    "            for jplot in range(1, 24):\n",
    "                plt.subplot(n_plot, 25, iplot * 25 + jplot + 1)\n",
    "                plt.imshow(X_fake[iplot, jplot, :, :].squeeze(), vmin=0, vmax=1, cmap=plt.cm.hot_r)\n",
    "                plt.axis('off')\n",
    "        plt.colorbar()\n",
    "        plt.suptitle(f'epoch {epoch:04d}')\n",
    "        plt.savefig(f'{plotdirs}/fake_samples_{epoch:04d}_{i:06d}.{plot_format}')#cambio i por j\n",
    "\n",
    "        # plot loss\n",
    "        plt.figure()\n",
    "        plt.plot(hist['d_loss'], label='d_loss')\n",
    "        plt.plot(hist['g_loss'], label='g_loss')\n",
    "        plt.ylabel('batch')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{plotdirs}/training_loss.{plot_format}')\n",
    "        pd.DataFrame(hist).to_csv('hist.csv')\n",
    "        plt.close('all')\n",
    "\n",
    "        generator.save(f'{outdirs}/gen_{epoch:04d}.h5')\n",
    "        critic.save(f'{outdirs}/disc_{epoch:04d}.h5')\n",
    "\n",
    "\n",
    "# the training is done with increasing batch size,\n",
    "# as defined in n_epoch_and_batch_size_list at the beginning of the script\n",
    "start_epoch = 0\n",
    "for n_epochs, batch_size in  n_epoch_and_batch_size_list:\n",
    "    train(n_epochs, batch_size, start_epoch)\n",
    "    start_epoch = start_epoch + n_epochs #this is only needed for correct plot labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffaf06a-e338-45db-8162-c1e9b6414790",
   "metadata": {},
   "source": [
    "## Evaluates the GAN and makes analysis plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a7a49-75e4-4ae1-a597-f2cf696982e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "finished loading data\n",
      "evaluate in 3 samples\n",
      "load the trained generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 2/2 [06:36<00:00, 198.26s/it]\n",
      "100%|ââââââââââ| 10000/10000 [09:17<00:00, 17.93it/s]\n",
      "Exception in thread Thread-48:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/pool.py\", line 412, in _handle_workers\n",
      "    pool._maintain_pool()\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/pool.py\", line 248, in _maintain_pool\n",
      "    self._repopulate_pool()\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/pool.py\", line 241, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/process.py\", line 112, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/popen_fork.py\", line 20, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/popen_fork.py\", line 70, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "Exception in thread Thread-45:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/pool.py\", line 412, in _handle_workers\n",
      "    pool._maintain_pool()\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/pool.py\", line 248, in _maintain_pool\n",
      "    self._repopulate_pool()\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/pool.py\", line 241, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/process.py\", line 112, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/popen_fork.py\", line 20, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/envs/pr-disagg-env/lib/python3.7/multiprocessing/popen_fork.py\", line 70, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#! /pfs/nobackup/home/s/sebsc/miniconda3/envs/pr-disagg-env/bin/python\n",
    "#SBATCH -A SNIC2019-3-611\n",
    "#SBATCH --time=06:00:00\n",
    "#SBATCH -N 1\n",
    "#SBATCH --exclusive\n",
    "\"\"\"\n",
    "this script uses the trained generator to create precipitation scenarios.\n",
    "a number of daily sum conditions are sampled from the test-data,\n",
    "and for each sub-daily scenarios are generated with the generator.\n",
    "The results are shown in various plots\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.colors as mcolors\n",
    "matplotlib.use('agg')\n",
    "from pylab import plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from tqdm import trange\n",
    "from skimage.util import view_as_windows\n",
    "from matplotlib.colors import LogNorm\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# for reproducability, we set a fixed seed to the random number generator\n",
    "np.random.seed(354)\n",
    "\n",
    "# we need to specify train start and enddate to get correct filenames\n",
    "train_startdate = '19700101'\n",
    "train_enddate = '19741231'\n",
    "\n",
    "eval_startdate = '19750101'\n",
    "eval_enddate = '19751231'\n",
    "\n",
    "\n",
    "# parameters (need to be the same as in training)\n",
    "ndomain = 16  # gridpoints\n",
    "stride = 16\n",
    "latent_dim = 100\n",
    "\n",
    "tp_thresh_daily = 40  # mm. in the radardate the unit is mm/h, but then on 5 minutes steps.\n",
    "# the conversion is done automatically in this script\n",
    "n_thresh = 80\n",
    "\n",
    "# here we need to choose which epoch we use from the saved models (we saved them at the end of every\n",
    "# epoch). visual inspection of the images generated from the training set showed\n",
    "# that after epoch 20, things starts to detoriate. Therefore we use epoch 20.\n",
    "epoch = 20\n",
    "# normalization of daily sums\n",
    "# we ues the 99.9 percentile of 2010\n",
    "#norm_scale = 127.4\n",
    "norm_scale = 1\n",
    "\n",
    "plot_format = 'png'\n",
    "\n",
    "name = 'results'\n",
    "\n",
    "plotdir = f'/home/jovyan/work/plots_generated_{name}_rev1/'\n",
    "\n",
    "outdir = f'/home/jovyan/work/trained_models/{name}/'\n",
    "\n",
    "# note for colab: sometimes mkdir does not work that way. in this case\n",
    "# you have to create the directories manually\n",
    "os.system(f'mkdir -p {plotdir}')\n",
    "os.system(f'mkdir -p {outdir}')\n",
    "\n",
    "# load data and precomputed indices for the test data\n",
    "\n",
    "converted_data_path = '/home/jovyan/work/prueba_datos_ERA5/'\n",
    "\n",
    "indices_data_path = '/home/jovyan/work/prueba_datos_ERA5'\n",
    "\n",
    "\n",
    "data_ifile = f'{converted_data_path}/prueba{startdate}-{enddate}.npy'\n",
    "\n",
    "\n",
    "indices_file = f'{indices_data_path}/valid_indices_ERA5-{startdate}-{enddate}.pkl'\n",
    "print('loading data')\n",
    "# load the data as memmap\n",
    "data = np.load(data_ifile, mmap_mode='r')\n",
    "\n",
    "indices_all = pickle.load(open(indices_file, 'rb'))\n",
    "# convert to array\n",
    "indices_all = np.array(indices_all)\n",
    "# this has shape (nsamples,3)\n",
    "# each row is (tidx,yidx,xidx)\n",
    "print('finished loading data')\n",
    "\n",
    "# the data has dimensions (sample,hourofday,x,y)\n",
    "n_days, nhours, ny, nx = data.shape\n",
    "n_channel = 1\n",
    "# sanity checks\n",
    "assert (len(data.shape) == 4)\n",
    "assert (len(indices_all.shape) == 2)\n",
    "assert (indices_all.shape[1] == 3)\n",
    "assert (nhours == 24 // 1)\n",
    "assert (np.max(indices_all[:, 0]) < n_days)\n",
    "assert (np.max(indices_all[:, 1]) < ny)\n",
    "assert (np.max(indices_all[:, 2]) < nx)\n",
    "assert (data.dtype == 'float32')\n",
    "\n",
    "n_samples = len(indices_all)\n",
    "\n",
    "print(f'evaluate in {n_samples} samples')\n",
    "\n",
    "print('load the trained generator')\n",
    "generator_file = f'{outdir}/gen_{epoch:04d}.h5'\n",
    "\n",
    "# we need the custom layer PixelNormalization to load the generator\n",
    "class PixelNormalization(tf.keras.layers.Layer):\n",
    "    # initialize the layer\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PixelNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    # perform the operation\n",
    "    def call(self, inputs):\n",
    "        # calculate square pixel values\n",
    "        values = inputs ** 2.0\n",
    "        # calculate the mean pixel values\n",
    "        mean_values = K.mean(values, axis=-1, keepdims=True)\n",
    "        # ensure the mean is not zero\n",
    "        mean_values += 1.0e-8\n",
    "        # calculate the sqrt of the mean squared value (L2 norm)\n",
    "        l2 = K.sqrt(mean_values)\n",
    "        # normalize values by the l2 norm\n",
    "        normalized = inputs / l2\n",
    "        return normalized\n",
    "\n",
    "    # define the output shape of the layer\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "gen = tf.keras.models.load_model(generator_file, compile=False,\n",
    "                                 custom_objects={'PixelNormalization': PixelNormalization})\n",
    "\n",
    "\n",
    "# in order to use the model, we need to compile it (even though we dont need the los function\n",
    "# and optimizer here, since we only do prediction)\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    # we use -1 for fake, and +1 for real labels\n",
    "    return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "gen.compile(loss=wasserstein_loss, optimizer=tf.keras.optimizers.RMSprop(lr=0.00005))\n",
    "\n",
    "\n",
    "def generate_real_samples_and_conditions(n_batch):\n",
    "    \"\"\"get random sampples and do the last preprocessing on them\"\"\"\n",
    "    # get random sample of indices from the precomputed indices\n",
    "    # for this we generate random indices for the index list (confusing termoonology, since we use\n",
    "    # indices to index the list of indices...\n",
    "    ixs = np.random.randint(n_samples, size=n_batch)\n",
    "    idcs_batch = indices_all[ixs]\n",
    "\n",
    "    # now we select the data corresponding to these indices\n",
    "    data_wview = view_as_windows(data, (1, 1, ndomain, ndomain))[..., 0, 0, :, :]\n",
    "    batch = data_wview[idcs_batch[:, 0], :, idcs_batch[:, 1], idcs_batch[:, 2]]\n",
    "    # add empty channel dimension (necessary for keras, which expects a channel dimension)\n",
    "    batch = np.expand_dims(batch, -1)\n",
    "    # compute daily sum (which is the condition)\n",
    "    batch_cond = np.sum(batch, axis=1)  # daily sum\n",
    "\n",
    "    # the data now is in mm/hour, but we want it as fractions of the daily sum for each day\n",
    "    for i in range(n_batch):\n",
    "        batch[i] = batch[i] / batch_cond[i]\n",
    "\n",
    "    # normalize daily sum\n",
    "    batch_cond = batch_cond / norm_scale\n",
    "    assert (batch.shape == (n_batch, nhours, ndomain, ndomain, 1))\n",
    "    assert (batch_cond.shape == (n_batch, ndomain, ndomain, 1))\n",
    "    assert (~np.any(np.isnan(batch)))\n",
    "    assert (~np.any(np.isnan(batch_cond)))\n",
    "    assert (np.max(batch) <= 1)\n",
    "    assert (np.min(batch) >= 0)\n",
    "\n",
    "    return [batch, batch_cond]\n",
    "\n",
    "\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "cmap = plt.cm.gist_earth_r\n",
    "plotnorm = LogNorm(vmin=0.01, vmax=50)\n",
    "\n",
    "# for each (real) condition, generate a couple of fake\n",
    "# distributions, and plot them all together\n",
    "\n",
    "n_to_generate = 20\n",
    "n_per_batch = 10\n",
    "n_batches = n_to_generate // n_per_batch\n",
    "n_fake_per_real = 10\n",
    "plotcount = 0\n",
    "for ibatch in trange(n_batches):\n",
    "\n",
    "    reals, conds = generate_real_samples_and_conditions(n_per_batch)\n",
    "\n",
    "    for real, cond in zip(reals, conds):\n",
    "        plotcount += 1\n",
    "        # for each cond, make several predictions with different latent noise\n",
    "        latent = np.random.normal(size=(n_fake_per_real, latent_dim))\n",
    "        # for efficiency reason, we dont make a single forecast with the network, but\n",
    "        # we batch all n_fake_per_real together\n",
    "        cond_batch = np.repeat(cond[np.newaxis], repeats=n_fake_per_real, axis=0)\n",
    "        generated = gen.predict([latent, cond_batch])\n",
    "\n",
    "\n",
    "        # make a matrix of mapplots.\n",
    "        # first column: condition (daily mean), the same for every row\n",
    "        # first row: real fractions per hour\n",
    "        # rest of the rows: generated fractions per hour, 1 row per realization\n",
    "        fig = plt.figure(figsize=(25, 12))\n",
    "        n_plot = n_fake_per_real + 1\n",
    "        ax = plt.subplot(n_plot, 25, 1)\n",
    "        # compute unnormalized daily sum. squeeze away empty channel dimension (for plotting)\n",
    "        dsum = cond.squeeze() * norm_scale\n",
    "        plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "        plt.axis('off')\n",
    "        ax.annotate('real', xy=(0, 0.5), xytext=(-5, 0), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='right', va='center', rotation='vertical')\n",
    "        ax.annotate(f'daily sum', xy=(0.5, 1), xytext=(0, 5), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='center', va='baseline')\n",
    "        for jplot in range(1, 24 + 1):\n",
    "            ax = plt.subplot(n_plot, 25, jplot + 1)\n",
    "            plt.imshow(real[jplot - 1, :, :].squeeze(), vmin=0, vmax=1, cmap=plt.cm.Greys)\n",
    "            plt.axis('off')\n",
    "            ax.annotate(f'{jplot:02d}'':00', xy=(0.5, 1), xytext=(0, 5),\n",
    "                        xycoords='axes fraction', textcoords='offset points',\n",
    "                        size='large', ha='center', va='baseline')\n",
    "        # plot fake samples\n",
    "        for iplot in range(n_fake_per_real):\n",
    "            plt.subplot(n_plot, 25, (iplot + 1) * 25 + 1)\n",
    "            plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "            plt.axis('off')\n",
    "            for jplot in range(1, 24 + 1):\n",
    "                plt.subplot(n_plot, 25, (iplot + 1) * 25 + jplot + 1)\n",
    "                im = plt.imshow(generated[iplot, jplot - 1, :, :].squeeze(), vmin=0, vmax=1, cmap=plt.cm.Greys)\n",
    "                plt.axis('off')\n",
    "        fig.subplots_adjust(right=0.93)\n",
    "        cbar_ax = fig.add_axes([0.93, 0.15, 0.007, 0.7])\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('fraction of daily precipitation', fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "        plt.savefig(f'{plotdir}/generated_fractions_{epoch:04d}_{plotcount:04d}_allhours.{plot_format}')\n",
    "\n",
    "        # now the same, but showing absolute precipitation fields\n",
    "        # compute absolute precipitation from fraction of daily sum.\n",
    "        # this can be done with numpy broadcasting.\n",
    "        # we also have to multiply with norm_scale (because cond is normalized)\n",
    "        generated_scaled = generated * cond * norm_scale\n",
    "\n",
    "        real_scaled = real * cond * norm_scale\n",
    "        fig = plt.figure(figsize=(25, 12))\n",
    "        # plot real one\n",
    "        ax = plt.subplot(n_plot, 25, 1)\n",
    "        im = plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "        plt.axis('off')\n",
    "        ax.annotate('real', xy=(0, 0.5), xytext=(-5, 0), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='right', va='center', rotation='vertical')\n",
    "        ax.annotate(f'daily sum', xy=(0.5, 1), xytext=(0, 5), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='center', va='baseline')\n",
    "\n",
    "        for jplot in range(1, 24 + 1):\n",
    "            ax = plt.subplot(n_plot, 25, jplot + 1)\n",
    "            plt.imshow(real_scaled[jplot - 1, :, :].squeeze(), cmap=cmap, norm=plotnorm)\n",
    "            plt.axis('off')\n",
    "            ax.annotate(f'{jplot:02d}'':00', xy=(0.5, 1), xytext=(0, 5),\n",
    "                        xycoords='axes fraction', textcoords='offset points',\n",
    "                        size='large', ha='center', va='baseline')\n",
    "        # plot fake samples\n",
    "        for iplot in range(n_fake_per_real):\n",
    "            plt.subplot(n_plot, 25, (iplot + 1) * 25 + 1)\n",
    "            plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "            plt.axis('off')\n",
    "            for jplot in range(1, 24 + 1):\n",
    "                plt.subplot(n_plot, 25, (iplot + 1) * 25 + jplot + 1)\n",
    "                plt.imshow(generated_scaled[iplot, jplot - 1, :, :].squeeze(), cmap=cmap, norm=plotnorm)\n",
    "                plt.axis('off')\n",
    "        fig.subplots_adjust(right=0.93)\n",
    "        cbar_ax = fig.add_axes([0.93, 0.15, 0.007, 0.7])\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('precipitation [mm]', fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=16)\n",
    "        plt.savefig(f'{plotdir}/generated_precip_{epoch:04d}_{plotcount:04d}_allhours.{plot_format}')\n",
    "\n",
    "        np.save(f'data/real_precip_for_mapplots_{plotcount}.npy', real_scaled)\n",
    "\n",
    "        # same as before, but only every 3rd hour.\n",
    "        # rest of the rows: generated fractions per 3rd hour, 1 row per realization\n",
    "        fig = plt.figure(figsize=(12, 12))\n",
    "        n_plot = n_fake_per_real + 1\n",
    "        ax = plt.subplot(n_plot, 9, 1)\n",
    "        # compute unnormalized daily sum. squeeze away empty channel dimension (for plotting)\n",
    "        dsum = cond.squeeze() * norm_scale\n",
    "        plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "        plt.axis('off')\n",
    "        ax.annotate('real', xy=(0, 0.5), xytext=(-5, 0), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='right', va='center', rotation='vertical')\n",
    "        ax.annotate(f'daily sum', xy=(0.5, 1), xytext=(0, 5), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='center', va='baseline')\n",
    "        for jplot in range(1, 8 + 1):\n",
    "            ax = plt.subplot(n_plot, 9, jplot + 1)\n",
    "            plt.imshow(real[jplot*3 - 1, :, :].squeeze(), vmin=0, vmax=1, cmap=plt.cm.Greys)\n",
    "            plt.axis('off')\n",
    "            hour = jplot*3\n",
    "            ax.annotate(f'{hour:02d}'':00', xy=(0.5, 1), xytext=(0, 5),\n",
    "                        xycoords='axes fraction', textcoords='offset points',\n",
    "                        size='large', ha='center', va='baseline')\n",
    "        # plot fake samples\n",
    "        for iplot in range(n_fake_per_real):\n",
    "            plt.subplot(n_plot, 8+1, (iplot + 1) * 9 + 1)\n",
    "            plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "            plt.axis('off')\n",
    "            for jplot in range(1, 8 + 1):\n",
    "                plt.subplot(n_plot, 9, (iplot + 1) * 9 + jplot + 1)\n",
    "                im = plt.imshow(generated[iplot, jplot*3 - 1, :, :].squeeze(), vmin=0, vmax=1, cmap=plt.cm.Greys)\n",
    "                plt.axis('off')\n",
    "        fig.subplots_adjust(right=0.93)\n",
    "        cbar_ax = fig.add_axes([0.93, 0.15, 0.007, 0.7])\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('fraction of daily precipitation', fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "        plt.savefig(f'{plotdir}/generated_fractions_{epoch:04d}_{plotcount:04d}.{plot_format}')\n",
    "\n",
    "        # now the same, but showing absolute precipitation fields\n",
    "        # compute absolute precipitation from fraction of daily sum.\n",
    "        # this can be done with numpy broadcasting.\n",
    "        # we also have to multiply with norm_scale (because cond is normalized)\n",
    "        generated_scaled = generated * cond * norm_scale\n",
    "\n",
    "        real_scaled = real * cond * norm_scale\n",
    "        fig = plt.figure(figsize=(12, 12))\n",
    "        n_plot = n_fake_per_real + 1\n",
    "        ax = plt.subplot(n_plot, 9, 1)\n",
    "        # compute unnormalized daily sum. squeeze away empty channel dimension (for plotting)\n",
    "        dsum = cond.squeeze() * norm_scale\n",
    "        plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "        plt.axis('off')\n",
    "        ax.annotate('real', xy=(0, 0.5), xytext=(-5, 0), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='right', va='center', rotation='vertical')\n",
    "        ax.annotate(f'daily sum', xy=(0.5, 1), xytext=(0, 5), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='center', va='baseline')\n",
    "        for jplot in range(1, 8 + 1):\n",
    "            ax = plt.subplot(n_plot, 9, jplot + 1)\n",
    "            plt.imshow(real_scaled[jplot*3 - 1, :, :].squeeze(), cmap=cmap, norm=plotnorm)\n",
    "            plt.axis('off')\n",
    "            hour = jplot*3\n",
    "            ax.annotate(f'{hour:02d}'':00', xy=(0.5, 1), xytext=(0, 5),\n",
    "                        xycoords='axes fraction', textcoords='offset points',\n",
    "                        size='large', ha='center', va='baseline')\n",
    "        # plot fake samples\n",
    "        for iplot in range(n_fake_per_real):\n",
    "            plt.subplot(n_plot, 8+1, (iplot + 1) * 9 + 1)\n",
    "            plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "            plt.axis('off')\n",
    "            for jplot in range(1, 8 + 1):\n",
    "                plt.subplot(n_plot, 9, (iplot + 1) * 9 + jplot + 1)\n",
    "                im = plt.imshow(generated_scaled[iplot, jplot*3 - 1, :, :].squeeze(), cmap=cmap, norm=plotnorm)\n",
    "                plt.axis('off')\n",
    "        fig.subplots_adjust(right=0.93)\n",
    "        cbar_ax = fig.add_axes([0.93, 0.15, 0.007, 0.7])\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('precipitation [mm]', fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=16)\n",
    "        plt.savefig(f'{plotdir}/generated_precip_{epoch:04d}_{plotcount:04d}.{plot_format}')\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "# compute statistics over\n",
    "# many generated smaples\n",
    "# we compute the areamean,\n",
    "n_sample = 10000\n",
    "amean_fraction_gen = []\n",
    "amean_fraction_real = []\n",
    "amean_gen = []\n",
    "amean_real = []\n",
    "dists_real = []\n",
    "dists_gen = []\n",
    "\n",
    "\n",
    "# for each real conditoin, we crate 1 fake sample\n",
    "for i in trange(n_sample):\n",
    "    real, cond = generate_real_samples_and_conditions(1)\n",
    "    latent = np.random.normal(size=(1, latent_dim))\n",
    "    generated = gen.predict([latent, cond])\n",
    "\n",
    "    generated = generated.squeeze()\n",
    "    real = real.squeeze()\n",
    "    cond = cond.squeeze()\n",
    "    # compute area means\n",
    "    amean_fraction_gen.append(np.mean(generated, axis=(1, 2)).squeeze())\n",
    "    amean_fraction_real.append(np.mean(real, axis=(1, 2)).squeeze())\n",
    "    amean_gen.append(np.mean(generated * cond * norm_scale, axis=(1, 2)).squeeze())\n",
    "    amean_real.append(np.mean(real * cond * norm_scale, axis=(1, 2)).squeeze())\n",
    "    dists_real.append(real * cond * norm_scale)\n",
    "    dists_gen.append(generated * cond * norm_scale)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "amean_fraction_gen = np.array(amean_fraction_gen)\n",
    "amean_fraction_real = np.array(amean_fraction_real)\n",
    "amean_gen = np.array(amean_gen)\n",
    "amean_real = np.array(amean_real)\n",
    "dists_gen = np.array(dists_gen)\n",
    "dists_real = np.array(dists_real)\n",
    "np.save('data/generated_samples.npy',dists_gen)\n",
    "np.save('data/real_samples.npy', dists_real)\n",
    "\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)\n",
    "    n = x.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    return(x, y)\n",
    "\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "# ecdf of area means. the hours are flattened\n",
    "plt.figure()\n",
    "ax1 = plt.subplot(211)\n",
    "plt.plot(*ecdf(amean_gen.flatten()), label='gen')\n",
    "plt.plot(*ecdf(amean_real.flatten()), label='real')\n",
    "plt.legend(loc='upper left')\n",
    "sns.despine()\n",
    "plt.xlabel('mm/h')\n",
    "plt.ylabel('ecdf areamean')\n",
    "plt.semilogx()\n",
    "# ecdf of (flattened) spatial data\n",
    "ax2 = plt.subplot(212)\n",
    "plt.plot(*ecdf(dists_gen.flatten()), label='gen')\n",
    "plt.plot(*ecdf(dists_real.flatten()), label='real')\n",
    "plt.legend(loc='upper left')\n",
    "sns.despine()\n",
    "plt.ylabel('ecdf')\n",
    "plt.xlabel('mm/h')\n",
    "plt.semilogx()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plotdir}/ecdf_allx_{epoch:04d}.png', dpi=400)\n",
    "# cut at 0.1mm/h\n",
    "ax1.set_xlim(xmin=0.5)\n",
    "ax1.set_ylim(ymin=0.8, ymax=1.01)\n",
    "ax2.set_xlim(xmin=0.1)\n",
    "ax2.set_ylim(ymin=0.6, ymax=1.01)\n",
    "plt.savefig(f'{plotdir}/ecdf_{epoch:04d}.png', dpi=400)\n",
    "\n",
    "plt.close('all')\n",
    "# free some memory\n",
    "del dists_gen\n",
    "del dists_real\n",
    "\n",
    "# convert to pandas data frame, with timeofday ('hour') as additional column\n",
    "res_df = []\n",
    "for i in range(24):\n",
    "    _df1 = pd.DataFrame({'fraction': amean_fraction_gen[:, i],\n",
    "                         'precip': amean_gen[:, i],\n",
    "                         'typ': 'generated',\n",
    "                         'hour': i + 1}, index=np.arange(len(amean_gen)))\n",
    "    _df2 = pd.DataFrame({'fraction': amean_fraction_real[:, i].squeeze(),\n",
    "                         'precip': amean_real[:, i],\n",
    "                         'typ': 'real',\n",
    "                         'hour': i + 1}, index=np.arange(len(amean_gen)))\n",
    "    res_df.append(_df1)\n",
    "    res_df.append(_df2)\n",
    "\n",
    "\n",
    "df = pd.concat(res_df)\n",
    "df.to_csv(f'{plotdir}/gen_and_real_ameans_{epoch:04d}.csv')\n",
    "\n",
    "# make boxplot\n",
    "for showfliers in (True, False):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(211)\n",
    "    sns.boxplot('hour', 'precip', data=df, hue='typ', showfliers=showfliers)\n",
    "    plt.xlabel('')\n",
    "    sns.despine()\n",
    "    plt.subplot(212)\n",
    "    sns.boxplot('hour', 'fraction', data=df, hue='typ', showfliers=showfliers)\n",
    "    sns.despine()\n",
    "    plt.suptitle(f'n={n_sample}')\n",
    "    plt.savefig(f'{plotdir}/daily_cycle_showfliers{showfliers}_{epoch:04d}.svg')\n",
    "\n",
    "\n",
    "## for a single real one, generate a large\n",
    "# number of fake distributions, and then\n",
    "# plot the areamean in a lineplot\n",
    "# we generate 100 fake distributions with different noise accross the samples\n",
    "# and additionally 10 fake ones that use the same noise for all plots\n",
    "# the latter we plot in the same color (1 seperate color for each generated one)\n",
    "# so that we can compare them accross the plots\n",
    "\n",
    "n_to_generate = 20\n",
    "n_fake_per_real = 100\n",
    "n_fake_per_real_samenoise = 10\n",
    "plotcount = 0\n",
    "hours = np.arange(1, 24 + 1)\n",
    "# use same noise for all samples\n",
    "latent_shared = np.random.normal(size=(n_fake_per_real_samenoise, latent_dim))\n",
    "for isample in trange(n_to_generate):\n",
    "    real, cond = generate_real_samples_and_conditions(1)\n",
    "    latent= np.random.normal(size=(n_fake_per_real, latent_dim))\n",
    "    # for efficiency reason, we dont make a single forecast with the network, but\n",
    "    # we batch all n_fake_per_real together\n",
    "    cond_batch = np.repeat(cond, repeats=n_fake_per_real, axis=0)\n",
    "    cond_batch_samenoise = np.repeat(cond, repeats=n_fake_per_real_samenoise, axis=0)\n",
    "    generated = gen.predict([latent, cond_batch], verbose=1)\n",
    "    generated_samenoise = gen.predict([latent_shared, cond_batch_samenoise], verbose=1)\n",
    "    real = real.squeeze()\n",
    "    generated = generated.squeeze()\n",
    "    generated_samenoise = generated_samenoise.squeeze()\n",
    "    # compute are mean\n",
    "    amean_real = np.mean(real * cond.squeeze() * norm_scale, (1, 2))\n",
    "    amean_gen = np.mean(generated * cond.squeeze() * norm_scale, (2, 3))  # generated has a time dimension\n",
    "    amean_gen_samenoise = np.mean(generated_samenoise * cond.squeeze() * norm_scale, (2, 3))  # generated has a time dimension\n",
    "\n",
    "    plt.figure(figsize=(7, 3))\n",
    "    plt.plot(hours, amean_gen.T, label='_nolegend_', alpha=0.3,color='#1b9e77')\n",
    "    plt.plot(hours, amean_gen_samenoise.T, label='_nolegend_', alpha=1)\n",
    "    plt.plot(hours, amean_real, label='real', color='black')\n",
    "    plt.xlabel('hour')\n",
    "    plt.ylabel('precipitation [mm/hour]')\n",
    "    plt.legend()\n",
    "    sns.despine()\n",
    "    plt.savefig(f'{plotdir}/distribution_lineplot_samenosie_{epoch:04d}_{isample:04d}.svg')\n",
    "    plt.close('all')\n",
    "\n",
    "# take two conditions, and\n",
    "# then plot the areamean of the resulting distributions, and check whether they are different\n",
    "# we use the same noise for both, to avoid finding effects that only might come from the noise\n",
    "n_fake_per_real = 1000\n",
    "latent = np.random.normal(size=(n_fake_per_real, latent_dim))\n",
    "for isample in trange(20):\n",
    "    real1, cond1 = generate_real_samples_and_conditions(1)\n",
    "\n",
    "    cond_batch1 = np.repeat(cond1, repeats=n_fake_per_real, axis=0)\n",
    "    generated1 = gen.predict([latent, cond_batch1], verbose=1)\n",
    "    real2, cond2 = generate_real_samples_and_conditions(1)\n",
    "    cond_batch2 = np.repeat(cond2, repeats=n_fake_per_real, axis=0)\n",
    "    generated2 = gen.predict([latent, cond_batch2], verbose=1)\n",
    "\n",
    "    amean_fraction_real1 = np.mean(real1.squeeze(), (1, 2)).squeeze()\n",
    "    amean_fraction_gen1 = np.mean(generated1, (2, 3)).squeeze()  # generated has a time dimension\n",
    "    amean_fraction_real2 = np.mean(real2.squeeze(), (1, 2)).squeeze()\n",
    "    amean_fraction_gen2 = np.mean(generated2.squeeze(), (2, 3)).squeeze()  # generated has a time dimension\n",
    "\n",
    "    res_df = []\n",
    "    for i in range(24):\n",
    "        _df1 = pd.DataFrame({'fraction': amean_fraction_gen1[:, i],\n",
    "                             'cond': 1,\n",
    "                             'hour': i + 1}, index=np.arange(len(amean_fraction_gen1)))\n",
    "        _df2 = pd.DataFrame({'fraction': amean_fraction_gen2[:, i],\n",
    "                             'cond': 2,\n",
    "                             'hour': i + 1}, index=np.arange(len(amean_fraction_gen1)))\n",
    "        res_df.append(_df1)\n",
    "        res_df.append(_df2)\n",
    "\n",
    "    df = pd.concat(res_df)\n",
    "    df.to_csv(f'{plotdir}/check_conditional_dist_samenoise_{epoch:04d}_{isample:04d}.csv')\n",
    "    pvals_per_hour = []\n",
    "    for hour in range(1,24+1):\n",
    "        sub = df.query('hour==@hour')\n",
    "        _, p = scipy.stats.ks_2samp(sub.query('cond==1')['fraction'], sub.query('cond==2')['fraction'])\n",
    "        pvals_per_hour.append(p)\n",
    "    np.savetxt(f'{plotdir}/check_conditional_dist_samenoise_KSpval_{epoch:04d}_{isample:04d}.txt', pvals_per_hour)\n",
    "    for showfliers in (True, False):\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(6, 4.8))\n",
    "        gs = fig.add_gridspec(2, 2)\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        im = ax1.imshow(cond1.squeeze(), cmap=cmap, norm=plotnorm)\n",
    "        plt.title('cond 1')\n",
    "        plt.axis('off')\n",
    "        plt.colorbar(im)\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        im = ax2.imshow(cond2.squeeze(), cmap=cmap, norm=plotnorm)\n",
    "        plt.title('cond 2')\n",
    "        plt.axis('off')\n",
    "        plt.colorbar(im)\n",
    "        ax3 = fig.add_subplot(gs[1, :])\n",
    "        sns.boxplot('hour', 'fraction', hue='cond', data=df, ax=ax3, showfliers=showfliers)\n",
    "        sns.despine()\n",
    "        plt.savefig(f'{plotdir}/check_conditional_dist_samenoise_showfliers{showfliers}_{epoch:04d}_{isample:04d}.svg')\n",
    "\n",
    "    plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pr-disagg-env",
   "language": "python",
   "name": "pr-disagg-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
