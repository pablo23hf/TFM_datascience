{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35ffbecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "outpath='/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos'\n",
    "os.system(f'mkdir -p {outpath}')\n",
    "for date in pd.date_range('20151215','20151215'):\n",
    "    url = f'https://opendata-download-radar.smhi.se/api/version/latest/area/sweden/product/comp/{date.year}/{date.month}/{date.day}.zip?format=tif'\n",
    "    outname = f'{outpath}/smhi_radar_{date.strftime(\"%Y%m%d\")}.zip'\n",
    "    os.system(f'wget -O {outname} {url}')\n",
    "\n",
    "\n",
    "os.system(' for f in smhi_radar*.zip; do unzip -o ${f}; rm ${f}; done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ddab7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='for f in smhi_radar*.zip; do unzip -o ${f}; rm ${f}; done', returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Cambiar al directorio deseado\n",
    "directorio = '/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos'\n",
    "os.chdir(directorio)\n",
    "\n",
    "# Ejecutar el comando en el directorio actual\n",
    "comando = \"for f in smhi_radar*.zip; do unzip -o ${f}; rm ${f}; done\"\n",
    "subprocess.run(comando, shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e423de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#! /climstorage/sebastian/anaconda3/envs/pr-disagg-env/bin/python\n",
    "\"\"\"\n",
    "convert the single .tif smhi radar files to daily netcdf files\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "path='/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos'\n",
    "outpath = f'{path}/netcdf/'\n",
    "os.system(f'mkdir -p {outpath}')\n",
    "\n",
    "\n",
    "dates = pd.date_range('20151215','20151215')\n",
    "failed_dates = [] # the radar data is not complete\n",
    "for date in tqdm(dates):\n",
    "    # the smhi radar has 5 minute timesteps\n",
    "    try:\n",
    "        res = []\n",
    "        for hour in range(0, 24):\n",
    "            for minute in range(0, 60, 5):\n",
    "                iname = f'{path}/radar_{date.strftime(\"%y%m%d\")}{hour:02}{minute:02}.tif'\n",
    "                da = xr.open_rasterio(iname)\n",
    "                da['time'] = pd.to_datetime(f'{date.strftime(\"%Y%m%d\")}{hour:02}{minute:02}')\n",
    "                res.append(da)\n",
    "\n",
    "        res = xr.concat(res, dim='time')\n",
    "        if res['band'].size >1:\n",
    "             # select first band (for data from 2015-2017 there are 3 bands, but quick\n",
    "             # inspeciton showed that they seem to have the same data\n",
    "            res = res.isel(band=0)\n",
    "        # remove empty band dimension\n",
    "        res = res.squeeze()\n",
    "\n",
    "        # replace missing vals with nan (missing value is 255)\n",
    "        res = res.where(res != 255)\n",
    "        # convert to mm/5mins (https://opendata.smhi.se/apidocs/radar/data.html)\n",
    "        dbz = res * 0.4 - 30\n",
    "        mmperh = ((10 ** (dbz / 10)) / 200) ** (1 / 1.5)\n",
    "        mmper5min = mmperh * 5/60\n",
    "        res = mmper5min\n",
    "        res.to_netcdf(f'{outpath}/smhi_radar_{date.strftime(\"%Y%m%d\")}.nc')\n",
    "\n",
    "    except:\n",
    "        print(f'date {date} failed, skipping')\n",
    "        failed_dates.append(date)\n",
    "\n",
    "print('failed_dates:')\n",
    "print(failed_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a7ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /proj/bolinc/users/x_sebsc/anaconda3/envs/pr-disagg-env/bin/python\n",
    "#SBATCH -N1\n",
    "#SBATCH -A snic2019-1-2\n",
    "#SBATCH -t 1:30:00\n",
    "#SBATCH --mem=363GB\n",
    "#! /climstorage/sebastian/anaconda3/envs/pr-disagg-env/bin/python\n",
    "\"\"\"\n",
    "this script reads in the netcdf radar data (output of convert_smhi_radardata.py),\n",
    "and converts it to a format suitable for training.\n",
    "\n",
    "the data is\n",
    "1) summed to the desired timeresolution \"tres\" (default 1 hour)\n",
    "2) reshaped in a format that has hour of the day as separate dimension\n",
    "    --> output format is (days,tperday,lat,lon)\n",
    "3) saved as a single .npz file\n",
    "\n",
    "\n",
    "note that this script is not very memory efficient (the whole dataset needs to be loaded into ram).\n",
    "if you dont have enough RAM, then it would be better to process each year individually\n",
    "\n",
    "@internal: run on tetralith\n",
    "\n",
    "@author: Sebastian Scher\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "\n",
    "\n",
    "#PARAMS\n",
    "# for training data:\n",
    "startdate = '20151215'\n",
    "enddate = '20151215'\n",
    "# for test data:\n",
    "# startdate = '20170101'\n",
    "# enddate = '20181231'\n",
    "tres=1 # [h]\n",
    "\n",
    "# END PARAMS\n",
    "# the radardata is 5 minute data, but in mm/h. so to get mm/day for the daysums,\n",
    "# we need to divide by 60/5=12\n",
    "conv_factor = 1/12\n",
    "\n",
    "datapath='/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos/netcdf/'\n",
    "\n",
    "outpath='/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos/preprocessed/'\n",
    "os.system(f'mkdir -p {outpath}')\n",
    "\n",
    "# create list of available files\n",
    "dates_all = pd.date_range(startdate,enddate,freq='1d')\n",
    "ifiles = []\n",
    "for date in dates_all:\n",
    "    fname = f'{datapath}/smhi_radar_{date.strftime(\"%Y%m%d\")}.nc'\n",
    "    if os.path.exists(fname):\n",
    "        ifiles.append(fname)\n",
    "\n",
    "if len(ifiles) == 0:\n",
    "    raise Exception('no input files found!')\n",
    "\n",
    "\n",
    "# now open all files lazily\n",
    "# they are automatically chunked per file (thus per day)\n",
    "data_raw = xr.open_mfdataset(ifiles, combine='nested', concat_dim='time')\n",
    "data_raw = data_raw['__xarray_dataarray_variable__']\n",
    "# convert to 32bit\n",
    "data_raw = data_raw.astype('float32')\n",
    "\n",
    "# sum to desired timeresolution\n",
    "agg = data_raw.resample(time=f'{tres}h', label='left').sum(skipna=False)\n",
    "# convert to numpy array\n",
    "agg = agg.values\n",
    "\n",
    "# now we want to reshape to (days,tperday,lat,lon)\n",
    "t_per_day = int(24/tres)\n",
    "\n",
    "ntime,ny,nx = agg.shape\n",
    "ndays = ntime / t_per_day\n",
    "assert(ndays.is_integer())\n",
    "ndays = int(ndays)\n",
    "reshaped = agg.reshape((ndays,t_per_day,ny,nx))\n",
    "\n",
    "final = reshaped\n",
    "\n",
    "np.savez_compressed(f'{outpath}/{startdate}-{enddate}_tres{tres}.npz',data=final)\n",
    "np.save(f'{outpath}/{startdate}-{enddate}_tres{tres}', final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d402e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7719f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 1\n",
      "found 47 valid samples\n"
     ]
    }
   ],
   "source": [
    "#! /climstorage/sebastian/anaconda3/envs/pr-disagg-env/bin/python\n",
    "\"\"\"\n",
    "\n",
    "read in formatted data (outut from reformat_data.py),\n",
    "and determined all valid training samples from it.\n",
    "\n",
    "valid training samples are all ndomain x ndomain boxes that are free of NaN values,\n",
    "and where the daily sum exceeds a certain threshold at at least a certain amount of points\n",
    "(default 5mm on 20 points on a 16x16 domain)\n",
    "\n",
    "the \"sweep\" over the domain is controlled by the \"stride\" parameter. If it is 1, then all possible boxes\n",
    "are tried out (including those with overlap). with strid=ndomain, all non-overlapping boxes are scanned.\n",
    "\n",
    "output: .pkl file containing the indices of the training samples\n",
    "\n",
    "these indices can then be used the following way:\n",
    "idcs = final_valid_idcs[0]\n",
    "sub = data[idcs[0],:,idcs[1]:idcs[1]+ndomain,idcs[2]:idcs[2]+ndomain]\n",
    "\n",
    "@internal: run on misu160 and kebnekaise\n",
    "\n",
    "@author: Sebastian Scher\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import numba\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()\n",
    "\n",
    "os.system('mkdir -p data')\n",
    "# the data is not complete (not all days are available)\n",
    "# PARAMS\n",
    "# for training data:\n",
    "startdate = '20191231'\n",
    "enddate = '20191231'\n",
    "# for test data:\n",
    "# startdate = '20170101'\n",
    "# enddate = '20181231'\n",
    "ndomain = 16  # gridpoints\n",
    "stride = 16  # |ndomain # in which steps to scan the whole domain\n",
    "tres = 1\n",
    "tp_thresh_daily = 5  # mm. in the radardate the unit is mm/h, but then on 5 minutes steps.\n",
    "# the conversion is done automatically in this script\n",
    "n_thresh = 20\n",
    "# END PARAMS\n",
    "\n",
    "if ndomain % 2 != 0:\n",
    "    raise ValueError(f'ndomain must be an even number')\n",
    "\n",
    "\n",
    "datapath = '/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos/preprocessed/'\n",
    "\n",
    "ifile = f'{datapath}/{startdate}-{enddate}_tres{tres}.npy'\n",
    "\n",
    "data = np.load(ifile, mmap_mode='r')\n",
    "\n",
    "if len(data.shape) != 4:\n",
    "    raise ValueError(f'data has wrong number of dimensions {len(data.shape)} instead of 4')\n",
    "\n",
    "# compute daily sum, which is the sum over the hour axis\n",
    "n_days,nhour, ny, nx = data.shape\n",
    "\n",
    "\n",
    "# compute all valid indices\n",
    "# for this, we try out all ndomain x ndomain squares shifted by strides, and check whether they have any missing data,\n",
    "# and if not, whether they adhere to the criteria set by tp_thresh_daily and n_thresh\n",
    "# since this contains many for loops, we speed it up with numba\n",
    "\n",
    "\n",
    "@numba.jit\n",
    "def filter(data):\n",
    "    final_valid_idcs = []\n",
    "    # loop over timeslices\n",
    "    for tidx in numba.prange(n_days):\n",
    "        print(tidx, '/', n_days)\n",
    "        # daily sum\n",
    "        sub = np.sum(data[tidx],axis=0)\n",
    "        # loop over all possible boxes\n",
    "        for ii in range(0, ny - ndomain, stride):\n",
    "            for jj in range(0, nx - ndomain, stride):\n",
    "                subsub = sub[ii:ii + ndomain, jj:jj + ndomain]\n",
    "                # check for nan values\n",
    "                if not np.any(np.isnan(subsub)):\n",
    "                    # if at least n_thresh points are above the threshold,\n",
    "                    # we use this box\n",
    "                    if np.sum(subsub > tp_thresh_daily) >= n_thresh:\n",
    "                        final_valid_idcs.append((tidx, ii, jj))\n",
    "\n",
    "    return final_valid_idcs\n",
    "\n",
    "\n",
    "final_valid_idcs = filter(data)\n",
    "\n",
    "params = f'{startdate}-{enddate}-tp_thresh_daily{tp_thresh_daily}_n_thresh{n_thresh}_ndomain{ndomain}_stride{stride}'\n",
    "pickle.dump(final_valid_idcs, open(f'/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos/preprocessed/valid_indices_smhi_radar_{params}.pkl', 'wb'))\n",
    "\n",
    "print(f'found {len(final_valid_idcs)} valid samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a05643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "finished loading data\n",
      "building networks\n",
      "finished building networks\n",
      "start training on 47 samples\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:22<00:00, 22.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 1/1, d_loss -0.0001294845133088529 g:0.0013032795395702124 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 1/50 [00:38<31:41, 38.80s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:19<00:00, 19.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, 1/1, d_loss -0.0026957744266837835 g:0.0022791922092437744 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 2/50 [01:16<30:26, 38.04s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "#! /pfs/nobackup/home/s/sebsc/miniconda3/envs/pr-disagg-env/bin/python\n",
    "#SBATCH -A SNIC2019-3-611\n",
    "#SBATCH --time=3-00:00:00\n",
    "#SBATCH --gres=gpu:v100:1\n",
    "\"\"\"\n",
    "training script for the network. it loads the data as memmap, so it does not need large amounts of RAM\n",
    "\n",
    "input: output from reformat_data.py and compute_valid_indices.py\n",
    "\n",
    "\n",
    "@internal: run on kebnekaise (using sbatch definitions on top of the file) and on colab. final run\n",
    "make on kebnekaise\n",
    "\n",
    "on colab add the following on top of the first cell:\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "terminology used here: the word \"generator\" is used both for the generator of the GAN, and for\n",
    "\"python generators\", which is a special type of iterable in python that we use here for feeding\n",
    "the input data into the network.\n",
    "\n",
    "@author: Sebastian Scher\n",
    "\n",
    "needs tensorflow >=2.1\n",
    "conda install tensorflow-gpu==2.1.0\n",
    "\n",
    "\"\"\"\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('agg')\n",
    "from pylab import plt\n",
    "from tqdm import trange\n",
    "from skimage.util import view_as_windows\n",
    "from matplotlib.colors import LogNorm\n",
    "from tensorflow.keras.utils import GeneratorEnqueuer\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "\n",
    "startdate = '20191231'\n",
    "enddate = '20191231'\n",
    "\n",
    "ndomain = 16  # gridpoints\n",
    "stride = 16\n",
    "tres = 1\n",
    "\n",
    "tp_thresh_daily = 5  # mm. in the radardate the unit is mm/h, but then on 5 minutes steps.\n",
    "# the conversion is done automatically in this script\n",
    "n_thresh = 20\n",
    "\n",
    "# normalization of daily sums\n",
    "# we ues the 99.9 percentile of 2010\n",
    "norm_scale = 127.4\n",
    "\n",
    "# neural network parameters\n",
    "n_disc = 5\n",
    "GRADIENT_PENALTY_WEIGHT = 10  # As per the paper\n",
    "latent_dim = 100\n",
    "batch_size = 32 # this is used as global variable in randomweightedaverage\n",
    "# the training is done with increasing batch size. each tuple is\n",
    "# a combination nof number of epochs and batch_size\n",
    "#n_epoch_and_batch_size_list = ((5, 32), (10, 64), (10, 128), (20, 256))\n",
    "n_epoch_and_batch_size_list = ((50, 32),)\n",
    "\n",
    "plot_format = 'png'\n",
    "\n",
    "name='wgancp_pixelnorm'\n",
    "\n",
    "# input and output directories. different for different machines\n",
    "if 'SNIC_RESOURCE' in os.environ.keys() and os.environ['SNIC_RESOURCE'] == 'kebnekaise':\n",
    "    machine = 'kebnekaise'\n",
    "else:\n",
    "    machine = 'colab'\n",
    "\n",
    "plotdirs ={'kebnekaise': f'/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/plots_{name}/',\n",
    "           'misu160': f'/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/plots_{name}/',\n",
    "           'colab':f'/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/plots_{name}/'}\n",
    "plotdir = plotdirs[machine]\n",
    "\n",
    "outdirs = {'kebnekaise': f'/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/trained_models/{name}/',\n",
    "           'misu160': f'/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/trained_models/{name}/',\n",
    "           'colab': f'/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/trained_models/{name}/'}\n",
    "outdir = outdirs[machine]\n",
    "# note for colab: sometimes mkdir does not work that way. in this case\n",
    "# you have to create the directories manually\n",
    "os.system(f'mkdir -p {plotdir}')\n",
    "os.system(f'mkdir -p {outdir}')\n",
    "\n",
    "# load data and precomputed indices\n",
    "\n",
    "converted_data_paths = {'misu160': '/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos/preprocessed/',\n",
    "                        'kebnekaise': '/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos/preprocessed/',\n",
    "                        'colab': '/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos/preprocessed/'}\n",
    "converted_data_path = converted_data_paths[machine]\n",
    "indices_data_paths = {'misu160': '/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos/preprocessed/',\n",
    "                      'kebnekaise': '/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos/preprocessed/',\n",
    "                      'colab': '/home/pablo/Physics/MasterDS/TFM/TFM_datascience/pruebas_GAN/datos/preprocessed/'}\n",
    "indices_data_path = indices_data_paths[machine]\n",
    "\n",
    "data_ifile = f'{converted_data_path}/{startdate}-{enddate}_tres{tres}.npy'\n",
    "\n",
    "params = f'{startdate}-{enddate}-tp_thresh_daily{tp_thresh_daily}_n_thresh{n_thresh}_ndomain{ndomain}_stride{stride}'\n",
    "indices_file = f'{indices_data_path}/valid_indices_smhi_radar_{params}.pkl'\n",
    "print('loading data')\n",
    "# load the data as memmap\n",
    "data = np.load(data_ifile, mmap_mode='r')\n",
    "\n",
    "\n",
    "indices_all = pickle.load(open(indices_file, 'rb'))\n",
    "# convert to array\n",
    "indices_all = np.array(indices_all)\n",
    "# this has shape (nsamples,3)\n",
    "# each row is (tidx,yidx,xidx)\n",
    "print('finished loading data')\n",
    "\n",
    "# the data has dimensions (sample,hourofday,x,y)\n",
    "n_days, nhours, ny, nx = data.shape\n",
    "n_channel=1\n",
    "# sanity checks\n",
    "assert (len(data.shape) == 4)\n",
    "assert (len(indices_all.shape) == 2)\n",
    "assert (indices_all.shape[1] == 3)\n",
    "assert (nhours == 24 // tres)\n",
    "assert (np.max(indices_all[:, 0]) < n_days)\n",
    "assert (np.max(indices_all[:, 1]) < ny)\n",
    "assert (np.max(indices_all[:, 2]) < nx)\n",
    "assert (data.dtype == 'float32')\n",
    "\n",
    "n_samples = len(indices_all)\n",
    "\n",
    "\n",
    "def generate_real_samples(n_batch):\n",
    "    \"\"\"get random sampples and do the last preprocessing on them\"\"\"\n",
    "    while True:\n",
    "        # get random sample of indices from the precomputed indices\n",
    "        # for this we generate random indices for the index list (confusing termoonology, since we use\n",
    "        # indices to index the list of indices...\n",
    "        ixs = np.random.randint(n_samples, size=n_batch)\n",
    "        idcs_batch = indices_all[ixs]\n",
    "\n",
    "        # now we select the data corresponding to these indices\n",
    "\n",
    "        data_wview = view_as_windows(data, (1, 1, ndomain, ndomain))[..., 0, 0, :,:]\n",
    "        batch = data_wview[idcs_batch[:, 0], :, idcs_batch[:, 1], idcs_batch[:, 2]]\n",
    "        # add empty channel dimension (necessary for keras, which expects a channel dimension)\n",
    "        batch = np.expand_dims(batch, -1)\n",
    "        # compute daily sum (which is the condition)\n",
    "        batch_cond = np.sum(batch, axis=1) # daily sum\n",
    "\n",
    "        # the data now is in mm/hour, but we want it as fractions of the daily sum for each day\n",
    "        for i in range(n_batch):\n",
    "            batch[i] = batch[i] / batch_cond[i]\n",
    "\n",
    "        # normalize daily sum\n",
    "        batch_cond = batch_cond / norm_scale\n",
    "        assert (batch.shape == (n_batch, nhours, ndomain, ndomain, 1))\n",
    "        assert (batch_cond.shape == (n_batch, ndomain, ndomain, 1))\n",
    "        assert (~np.any(np.isnan(batch)))\n",
    "        assert (~np.any(np.isnan(batch_cond)))\n",
    "        assert (np.max(batch) <= 1)\n",
    "        assert (np.min(batch) >= 0)\n",
    "\n",
    "        yield [batch, batch_cond]\n",
    "\n",
    "\n",
    "def generate_latent_points(n_batch):\n",
    "    # generate points in the latent space and a random condition\n",
    "    latent = np.random.normal(size=(n_batch, latent_dim))\n",
    "    # randomly select conditions\n",
    "    ixs = np.random.randint(0, n_samples, size=n_batch)\n",
    "    idcs_batch = indices_all[ixs]\n",
    "\n",
    "    data_wview = view_as_windows(data, (1, 1, ndomain, ndomain))[..., 0, 0, :,:]\n",
    "    batch = data_wview[idcs_batch[:, 0], :, idcs_batch[:, 1], idcs_batch[:, 2]]\n",
    "    # add empty channel dimension (necessary for keras, which expects a channel dimension)\n",
    "    batch = np.expand_dims(batch, -1)\n",
    "    batch_cond = np.sum(batch, axis=1) # daily sum\n",
    "    # normalize daily sum\n",
    "    batch_cond = batch_cond / norm_scale\n",
    "    assert (batch_cond.shape == (n_batch, ndomain, ndomain, 1))\n",
    "    assert (~np.any(np.isnan(batch_cond)))\n",
    "    return [latent, batch_cond]\n",
    "\n",
    "\n",
    "def generate_latent_points_as_generator(n_batch):\n",
    "    while True:\n",
    "        yield generate_latent_points(n_batch)\n",
    "\n",
    "\n",
    "def generate_fake_samples(n_batch):\n",
    "    # generate points in latent space\n",
    "    latent, cond = generate_latent_points(n_batch)\n",
    "    # predict outputs\n",
    "    generated = generator.predict([latent, cond])\n",
    "    return [generated, cond]\n",
    "\n",
    "\n",
    "def generate(cond):\n",
    "    latent = np.random.normal(size=(1, latent_dim))\n",
    "    cond = np.expand_dims(cond, 0)\n",
    "    return generator.predict([latent, cond])\n",
    "\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "\n",
    "class RandomWeightedAverage(tf.keras.layers.Layer):\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        global batch_size\n",
    "        alpha = tf.random.uniform((batch_size,1, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "\n",
    "\n",
    "class GradientPenalty(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GradientPenalty, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        super(GradientPenalty, self).build(input_shapes)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, inputs):\n",
    "        target, wrt = inputs\n",
    "        grad = K.gradients(target, wrt)[0]\n",
    "        return K.sqrt(K.sum(K.batch_flatten(K.square(grad)), axis=1, keepdims=True))-1\n",
    "\n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        return (input_shapes[1][0], 1)\n",
    "\n",
    "\n",
    "# pixel-wise feature vector normalization layer\n",
    "# from https://machinelearningmastery.com/how-to-train-a-progressive-growing-gan-in-keras-for-synthesizing-faces/\n",
    "class PixelNormalization(tf.keras.layers.Layer):\n",
    "    # initialize the layer\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PixelNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    # perform the operation\n",
    "    def call(self, inputs):\n",
    "        # calculate square pixel values\n",
    "        values = inputs ** 2.0\n",
    "        # calculate the mean pixel values\n",
    "        mean_values = K.mean(values, axis=-1, keepdims=True)\n",
    "        # ensure the mean is not zero\n",
    "        mean_values += 1.0e-8\n",
    "        # calculate the sqrt of the mean squared value (L2 norm)\n",
    "        l2 = K.sqrt(mean_values)\n",
    "        # normalize values by the l2 norm\n",
    "        normalized = inputs / l2\n",
    "        return normalized\n",
    "\n",
    "    # define the output shape of the layer\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def create_discriminator():\n",
    "    # we add the condition as additional channel. For this we\n",
    "    # expand its dimensions alon the nhours axis via linear scaling\n",
    "    in_cond = tf.keras.layers.Input(shape=(ndomain, ndomain, 1))\n",
    "    # add nhours dimension (size 1 for now)\n",
    "    cond_expanded = tf.keras.layers.Reshape((1, ndomain, ndomain, 1))(in_cond)\n",
    "    cond_expanded = tf.keras.layers.Lambda(lambda x: tf.keras.backend.repeat_elements(x, rep=nhours, axis=1))(\n",
    "        cond_expanded)\n",
    "    in_sample = tf.keras.layers.Input(shape=(nhours, ndomain, ndomain, 1))\n",
    "\n",
    "    in_combined = tf.keras.layers.Concatenate(axis=-1)([in_sample, cond_expanded])\n",
    "    kernel_size = (3, 3, 3)\n",
    "    main_net = tf.keras.Sequential([\n",
    "\n",
    "        tf.keras.layers.Conv3D(64, kernel_size=kernel_size, strides=2, input_shape=(nhours, ndomain, ndomain, 2),\n",
    "                               padding=\"valid\"),  # 11x7x7x32\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv3D(128, kernel_size=kernel_size, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv3D(256, kernel_size=kernel_size, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv3D(256, kernel_size=kernel_size, strides=2, padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(1, activation='linear'),\n",
    "    ])\n",
    "    out = main_net(in_combined)\n",
    "    model = tf.keras.Model(inputs=[in_sample, in_cond], outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_generator():\n",
    "\n",
    "    # for the moment, the flat approach is used\n",
    "    init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "    # define model\n",
    "\n",
    "    n_nodes = 256 * 2 * 2 * 3\n",
    "    in_latent = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "    # the condition is a 2d array (ndomain x ndomain), we simply flatten it\n",
    "    in_cond = tf.keras.layers.Input(shape=(ndomain, ndomain, n_channel))\n",
    "    in_cond_flat = tf.keras.layers.Flatten()(in_cond)\n",
    "    in_combined = tf.keras.layers.Concatenate()([in_latent, in_cond_flat])\n",
    "\n",
    "    main_net = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(n_nodes, kernel_initializer=init),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        tf.keras.layers.Reshape((3, 2, 2, 256)),\n",
    "\n",
    "        tf.keras.layers.UpSampling3D(size=(2, 2, 2)),\n",
    "        tf.keras.layers.Conv3D(256, (3, 3, 3), padding='same', kernel_initializer=init),\n",
    "        PixelNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "        tf.keras.layers.UpSampling3D(size=(2, 2, 2)),\n",
    "        tf.keras.layers.Conv3D(128, (3, 3, 3), padding='same', kernel_initializer=init),\n",
    "        PixelNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "\n",
    "        tf.keras.layers.UpSampling3D(size=(2, 2, 2)),\n",
    "        tf.keras.layers.Conv3D(64, (3, 3, 3), padding='same', kernel_initializer=init),\n",
    "        PixelNormalization(),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        # output 24x16x16x1\n",
    "        tf.keras.layers.Conv3D(1, (3, 3, 3), activation='linear', padding='same', kernel_initializer=init),\n",
    "        # softmax per gridpoint, thus over nhours, which is axis 1 (Softmax also counts the batch axis)\n",
    "        tf.keras.layers.Softmax(axis=1),\n",
    "        # check for Nans (only for debugging)\n",
    "        tf.keras.layers.Lambda(\n",
    "            lambda x: tf.debugging.check_numerics(x, 'found nan in output of per_gridpoint_softmax')),\n",
    "\n",
    "    ])\n",
    "\n",
    "    out = main_net(in_combined)\n",
    "    model = tf.keras.Model(inputs=[in_latent, in_cond], outputs=out)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print('building networks')\n",
    "generator = create_generator()\n",
    "critic = create_discriminator()\n",
    "generator.trainable = False\n",
    "# Image input (real sample)\n",
    "real_img = tf.keras.layers.Input(shape=(nhours,ndomain,ndomain,n_channel))\n",
    "# Noise input\n",
    "z_disc = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "# Generate image based of noise (fake sample) and add label to the input\n",
    "label = tf.keras.layers.Input(shape=(ndomain, ndomain, n_channel))\n",
    "fake_img = generator([z_disc, label])\n",
    "# Discriminator determines validity of the real and fake images\n",
    "fake = critic([fake_img, label])\n",
    "valid = critic([real_img, label])\n",
    "\n",
    "# Construct weighted average between real and fake images\n",
    "interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "\n",
    "# Determine validity of weighted sample\n",
    "validity_interpolated = critic([interpolated_img, label])\n",
    "# here we use the approach from https://github.com/jleinonen/geogan/blob/master/geogan/gan.py,\n",
    "# where gradient panely is a keras layer, and then 'mse' used as loss for this output\n",
    "disc_gp = GradientPenalty()([validity_interpolated, interpolated_img])\n",
    "\n",
    "# default from https://arxiv.org/pdf/1704.00028.pdf\n",
    "optimizer = tf.optimizers.Adam(lr=0.0001, beta_1=0, beta_2=0.9)\n",
    "\n",
    "critic_model = tf.keras.Model(inputs=[real_img, label, z_disc], outputs=[valid, fake, disc_gp])\n",
    "critic_model.compile(loss=[wasserstein_loss,\n",
    "                                wasserstein_loss,\n",
    "                                'mse'],\n",
    "                          optimizer=optimizer,\n",
    "                          loss_weights=[1, 1, 10])\n",
    "\n",
    "# For the generator we freeze the critic's layers\n",
    "critic.trainable = False\n",
    "generator.trainable = True\n",
    "\n",
    "# Sampled noise for input to generator\n",
    "z_gen = Input(shape=(latent_dim,))\n",
    "# add label to the input\n",
    "label = tf.keras.layers.Input(shape=(ndomain, ndomain, n_channel))\n",
    "# Generate images based of noise\n",
    "img = generator([z_gen, label])\n",
    "# Discriminator determines validity\n",
    "valid = critic([img, label])\n",
    "# Defines generator model\n",
    "generator_model = tf.keras.Model([z_gen, label], valid)\n",
    "generator_model.compile(loss=wasserstein_loss, optimizer=optimizer)\n",
    "print('finished building networks')\n",
    "\n",
    "# plot some real samples\n",
    "# plot a couple of samples\n",
    "plt.figure(figsize=(25, 25))\n",
    "n_plot = 30\n",
    "[X_real, cond_real] = next(generate_real_samples(n_plot))\n",
    "for i in range(n_plot):\n",
    "    plt.subplot(n_plot, 25, i * 25 + 1)\n",
    "    plt.imshow(cond_real[i, :, :].squeeze(), cmap=plt.cm.gist_earth_r, norm=LogNorm(vmin=0.01, vmax=1))\n",
    "    plt.axis('off')\n",
    "    for j in range(1, 24):\n",
    "        plt.subplot(n_plot, 25, i * 25 + j + 1)\n",
    "        plt.imshow(X_real[i, j, :, :].squeeze(), vmin=0, vmax=1, cmap=plt.cm.hot_r)\n",
    "        plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.savefig(f'{plotdir}/real_samples.{plot_format}')\n",
    "\n",
    "hist = {'d_loss': [], 'g_loss': []}\n",
    "print(f'start training on {n_samples} samples')\n",
    "\n",
    "\n",
    "def train(n_epochs, _batch_size, start_epoch=0):\n",
    "    \"\"\"\n",
    "        train with fixed batch_size for given epochs\n",
    "        make some example plots and save model after each epoch\n",
    "    \"\"\"\n",
    "    global batch_size\n",
    "    batch_size = _batch_size\n",
    "    # create a dataqueue with the keras facilities. this allows\n",
    "    # to prepare the data in parallel to the training\n",
    "    sample_dataqueue = GeneratorEnqueuer(generate_real_samples(batch_size),\n",
    "                                         use_multiprocessing=True)\n",
    "    sample_dataqueue.start(workers=2, max_queue_size=10)\n",
    "    sample_gen = sample_dataqueue.get()\n",
    "\n",
    "    # targets for loss function\n",
    "    gan_sample_dataqueue = GeneratorEnqueuer(generate_latent_points_as_generator(batch_size),\n",
    "                                         use_multiprocessing=True)\n",
    "    gan_sample_dataqueue.start(workers=2, max_queue_size=10)\n",
    "    gan_sample_gen = gan_sample_dataqueue.get()\n",
    "\n",
    "    # targets for loss function\n",
    "    valid = -np.ones((batch_size, 1))\n",
    "    fake = np.ones((batch_size, 1))\n",
    "    dummy = np.zeros((batch_size, 1))  # Dummy gt for gradient penalty\n",
    "\n",
    "    bat_per_epo = int(n_samples / batch_size)\n",
    "\n",
    "    # we need to call the discriminator once in order\n",
    "    # to initialize the input shapes\n",
    "    [X_real, cond_real] = next(sample_gen)\n",
    "    latent = np.random.normal(size=(batch_size, latent_dim))\n",
    "    critic_model.predict([X_real, cond_real, latent])\n",
    "    for i in trange(n_epochs):\n",
    "        epoch = 1 + i + start_epoch\n",
    "        # enumerate batches over the training set\n",
    "        for j in trange(bat_per_epo):\n",
    "\n",
    "            for _ in range(n_disc):\n",
    "                # fetch a batch from the queue\n",
    "                [X_real, cond_real] = next(sample_gen)\n",
    "                latent = np.random.normal(size=(batch_size, latent_dim))\n",
    "                d_loss = critic_model.train_on_batch([X_real, cond_real,latent], [valid, fake, dummy])\n",
    "                # we get for losses back here. average, valid, fake, and gradient_penalty\n",
    "                # we want the average of valid and fake\n",
    "                d_loss = np.mean([d_loss[1], d_loss[2]])\n",
    "\n",
    "\n",
    "            # train generator\n",
    "            # prepare points in latent space as input for the generator\n",
    "            [latent, cond] = next(gan_sample_gen)\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = generator_model.train_on_batch([latent, cond], valid)\n",
    "            # summarize loss on this batch\n",
    "            print(f'{epoch}, {j + 1}/{bat_per_epo}, d_loss {d_loss}' + \\\n",
    "                  f' g:{g_loss} ')  # , d_fake:{d_loss_fake} d_real:{d_loss_real}')\n",
    "\n",
    "            if np.isnan(g_loss) or np.isnan(d_loss):\n",
    "                raise ValueError('encountered nan in g_loss and/or d_loss')\n",
    "\n",
    "            hist['d_loss'].append(d_loss)\n",
    "            hist['g_loss'].append(g_loss)\n",
    "\n",
    "\n",
    "        # plot generated examples\n",
    "        plt.figure(figsize=(25, 25))\n",
    "        n_plot = 30\n",
    "        X_fake, cond_fake = generate_fake_samples(n_plot)\n",
    "        for iplot in range(n_plot):\n",
    "            plt.subplot(n_plot, 25, iplot * 25 + 1)\n",
    "            plt.imshow(cond_fake[iplot, :, :].squeeze(), cmap=plt.cm.gist_earth_r, norm=LogNorm(vmin=0.01, vmax=1))\n",
    "            plt.axis('off')\n",
    "            for jplot in range(1, 24):\n",
    "                plt.subplot(n_plot, 25, iplot * 25 + jplot + 1)\n",
    "                plt.imshow(X_fake[iplot, jplot, :, :].squeeze(), vmin=0, vmax=1, cmap=plt.cm.hot_r)\n",
    "                plt.axis('off')\n",
    "        plt.colorbar()\n",
    "        plt.suptitle(f'epoch {epoch:04d}')\n",
    "        plt.savefig(f'{plotdir}/fake_samples_{params}_{epoch:04d}_{j:06d}.{plot_format}')\n",
    "\n",
    "        # plot loss\n",
    "        plt.figure()\n",
    "        plt.plot(hist['d_loss'], label='d_loss')\n",
    "        plt.plot(hist['g_loss'], label='g_loss')\n",
    "        plt.ylabel('batch')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{plotdir}/training_loss_{params}.{plot_format}')\n",
    "        pd.DataFrame(hist).to_csv('hist.csv')\n",
    "        plt.close('all')\n",
    "\n",
    "        generator.save(f'{outdir}/gen_{params}_{epoch:04d}.h5')\n",
    "        critic.save(f'{outdir}/disc_{params}_{epoch:04d}.h5')\n",
    "\n",
    "\n",
    "# the training is done with increasing batch size,\n",
    "# as defined in n_epoch_and_batch_size_list at the beginning of the script\n",
    "start_epoch = 0\n",
    "for n_epochs, batch_size in  n_epoch_and_batch_size_list:\n",
    "    train(n_epochs, batch_size, start_epoch)\n",
    "    start_epoch = start_epoch + n_epochs #this is only needed for correct plot labelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e886ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /pfs/nobackup/home/s/sebsc/miniconda3/envs/pr-disagg-env/bin/python\n",
    "#SBATCH -A SNIC2019-3-611\n",
    "#SBATCH --time=06:00:00\n",
    "#SBATCH -N 1\n",
    "#SBATCH --exclusive\n",
    "\"\"\"\n",
    "this script uses the trained generator to create precipitation scenarios.\n",
    "a number of daily sum conditions are sampled from the test-data,\n",
    "and for each sub-daily scenarios are generated with the generator.\n",
    "The results are shown in various plots\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.colors as mcolors\n",
    "matplotlib.use('agg')\n",
    "from pylab import plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from tqdm import trange\n",
    "from skimage.util import view_as_windows\n",
    "from matplotlib.colors import LogNorm\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# for reproducability, we set a fixed seed to the random number generator\n",
    "np.random.seed(354)\n",
    "\n",
    "# we need to specify train start and enddate to get correct filenames\n",
    "train_startdate = '20090101'\n",
    "train_enddate = '20161231'\n",
    "\n",
    "eval_startdate = '20170101'\n",
    "eval_enddate = '20181231'\n",
    "\n",
    "# parameters (need to be the same as in training)\n",
    "ndomain = 16  # gridpoints\n",
    "stride = 16\n",
    "tres = 1\n",
    "latent_dim = 100\n",
    "\n",
    "tp_thresh_daily = 5  # mm. in the radardate the unit is mm/h, but then on 5 minutes steps.\n",
    "# the conversion is done automatically in this script\n",
    "n_thresh = 20\n",
    "\n",
    "# here we need to choose which epoch we use from the saved models (we saved them at the end of every\n",
    "# epoch). visual inspection of the images generated from the training set showed\n",
    "# that after epoch 20, things starts to detoriate. Therefore we use epoch 20.\n",
    "epoch = 20\n",
    "# normalization of daily sums\n",
    "# we ues the 99.9 percentile of 2010\n",
    "norm_scale = 127.4\n",
    "\n",
    "plot_format = 'png'\n",
    "\n",
    "name = 'wgancp_pixelnorm'\n",
    "\n",
    "# input and output directories. different for different machines\n",
    "machine = 'tiberino'\n",
    "\n",
    "\n",
    "plotdirs = {'kebnekaise': f'plots_generated_{name}_rev1/',\n",
    "            'tiberino': f'plots_generated_{name}_rev1/',\n",
    "            'misu160': f'plots_generated_{name}_rev1/',\n",
    "            'colab': f'/content/drive/My Drive/data/smhi_radar/plots_generated_{name}_rev1/'}\n",
    "plotdir = plotdirs[machine]\n",
    "\n",
    "outdirs = {'kebnekaise': f'/pfs/nobackup/home/s/sebsc/pr_disagg/trained_models/{name}/',\n",
    "           'tiberino': f'/data/fyris/sebastian/pr_disagg/from_keb/pr_disagg/trained_models/{name}/',\n",
    "           'misu160': f'/climstorage/sebastian/pr_disagg/smhi/rained_models/{name}/',\n",
    "           'colab': f'/content/drive/My Drive/data/smhi_radar/trained_models/{name}/'}\n",
    "outdir = outdirs[machine]\n",
    "# note for colab: sometimes mkdir does not work that way. in this case\n",
    "# you have to create the directories manually\n",
    "os.system(f'mkdir -p {plotdir}')\n",
    "os.system(f'mkdir -p {outdir}')\n",
    "\n",
    "# load data and precomputed indices for the test data\n",
    "\n",
    "converted_data_paths = {'misu160': '/climstorage/sebastian/pr_disagg/smhi/preprocessed/',\n",
    "                        'kebnekaise': '/home/s/sebsc/pfs/pr_disagg/smhi_radar/preprocessed',\n",
    "                        'tiberino': '/data/fyris/sebastian/pr_disagg/from_keb/pr_disagg/smhi_radar/preprocessed',\n",
    "                        'colab': '/content/drive/My Drive/data/smhi_radar/preprocessed/'}\n",
    "converted_data_path = converted_data_paths[machine]\n",
    "indices_data_paths = {'misu160': 'data/',\n",
    "                      'kebnekaise': 'data/',\n",
    "                      'tiberino': 'data/',\n",
    "                      'colab': '/content/drive/My Drive/data/smhi_radar/preprocessed/'}\n",
    "indices_data_path = indices_data_paths[machine]\n",
    "\n",
    "data_ifile = f'{converted_data_path}/{eval_startdate}-{eval_enddate}_tres{tres}.npy'\n",
    "\n",
    "params = f'{train_startdate}-{train_enddate}-tp_thresh_daily{tp_thresh_daily}_n_thresh{n_thresh}_ndomain{ndomain}_stride{stride}'\n",
    "params_eval = f'{eval_startdate}-{eval_enddate}-tp_thresh_daily{tp_thresh_daily}_n_thresh{n_thresh}_ndomain{ndomain}_stride{stride}'\n",
    "indices_file = f'{indices_data_path}/valid_indices_smhi_radar_{params_eval}.pkl'\n",
    "print('loading data')\n",
    "# load the data as memmap\n",
    "data = np.load(data_ifile, mmap_mode='r')\n",
    "\n",
    "indices_all = pickle.load(open(indices_file, 'rb'))\n",
    "# convert to array\n",
    "indices_all = np.array(indices_all)\n",
    "# this has shape (nsamples,3)\n",
    "# each row is (tidx,yidx,xidx)\n",
    "print('finished loading data')\n",
    "\n",
    "# the data has dimensions (sample,hourofday,x,y)\n",
    "n_days, nhours, ny, nx = data.shape\n",
    "n_channel = 1\n",
    "# sanity checks\n",
    "assert (len(data.shape) == 4)\n",
    "assert (len(indices_all.shape) == 2)\n",
    "assert (indices_all.shape[1] == 3)\n",
    "assert (nhours == 24 // tres)\n",
    "assert (np.max(indices_all[:, 0]) < n_days)\n",
    "assert (np.max(indices_all[:, 1]) < ny)\n",
    "assert (np.max(indices_all[:, 2]) < nx)\n",
    "assert (data.dtype == 'float32')\n",
    "\n",
    "n_samples = len(indices_all)\n",
    "\n",
    "print(f'evaluate in {n_samples} samples')\n",
    "\n",
    "print('load the trained generator')\n",
    "generator_file = f'{outdir}/gen_{params}_{epoch:04d}.h5'\n",
    "\n",
    "# we need the custom layer PixelNormalization to load the generator\n",
    "class PixelNormalization(tf.keras.layers.Layer):\n",
    "    # initialize the layer\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PixelNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    # perform the operation\n",
    "    def call(self, inputs):\n",
    "        # calculate square pixel values\n",
    "        values = inputs ** 2.0\n",
    "        # calculate the mean pixel values\n",
    "        mean_values = K.mean(values, axis=-1, keepdims=True)\n",
    "        # ensure the mean is not zero\n",
    "        mean_values += 1.0e-8\n",
    "        # calculate the sqrt of the mean squared value (L2 norm)\n",
    "        l2 = K.sqrt(mean_values)\n",
    "        # normalize values by the l2 norm\n",
    "        normalized = inputs / l2\n",
    "        return normalized\n",
    "\n",
    "    # define the output shape of the layer\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "gen = tf.keras.models.load_model(generator_file, compile=False,\n",
    "                                 custom_objects={'PixelNormalization': PixelNormalization})\n",
    "\n",
    "\n",
    "# in order to use the model, we need to compile it (even though we dont need the los function\n",
    "# and optimizer here, since we only do prediction)\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    # we use -1 for fake, and +1 for real labels\n",
    "    return tf.reduce_mean(y_true * y_pred)\n",
    "\n",
    "gen.compile(loss=wasserstein_loss, optimizer=tf.keras.optimizers.RMSprop(lr=0.00005))\n",
    "\n",
    "\n",
    "def generate_real_samples_and_conditions(n_batch):\n",
    "    \"\"\"get random sampples and do the last preprocessing on them\"\"\"\n",
    "    # get random sample of indices from the precomputed indices\n",
    "    # for this we generate random indices for the index list (confusing termoonology, since we use\n",
    "    # indices to index the list of indices...\n",
    "    ixs = np.random.randint(n_samples, size=n_batch)\n",
    "    idcs_batch = indices_all[ixs]\n",
    "\n",
    "    # now we select the data corresponding to these indices\n",
    "    data_wview = view_as_windows(data, (1, 1, ndomain, ndomain))[..., 0, 0, :, :]\n",
    "    batch = data_wview[idcs_batch[:, 0], :, idcs_batch[:, 1], idcs_batch[:, 2]]\n",
    "    # add empty channel dimension (necessary for keras, which expects a channel dimension)\n",
    "    batch = np.expand_dims(batch, -1)\n",
    "    # compute daily sum (which is the condition)\n",
    "    batch_cond = np.sum(batch, axis=1)  # daily sum\n",
    "\n",
    "    # the data now is in mm/hour, but we want it as fractions of the daily sum for each day\n",
    "    for i in range(n_batch):\n",
    "        batch[i] = batch[i] / batch_cond[i]\n",
    "\n",
    "    # normalize daily sum\n",
    "    batch_cond = batch_cond / norm_scale\n",
    "    assert (batch.shape == (n_batch, nhours, ndomain, ndomain, 1))\n",
    "    assert (batch_cond.shape == (n_batch, ndomain, ndomain, 1))\n",
    "    assert (~np.any(np.isnan(batch)))\n",
    "    assert (~np.any(np.isnan(batch_cond)))\n",
    "    assert (np.max(batch) <= 1)\n",
    "    assert (np.min(batch) >= 0)\n",
    "\n",
    "    return [batch, batch_cond]\n",
    "\n",
    "\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "cmap = plt.cm.gist_earth_r\n",
    "plotnorm = LogNorm(vmin=0.01, vmax=50)\n",
    "\n",
    "# for each (real) condition, generate a couple of fake\n",
    "# distributions, and plot them all together\n",
    "\n",
    "n_to_generate = 20\n",
    "n_per_batch = 10\n",
    "n_batches = n_to_generate // n_per_batch\n",
    "n_fake_per_real = 10\n",
    "plotcount = 0\n",
    "for ibatch in trange(n_batches):\n",
    "\n",
    "    reals, conds = generate_real_samples_and_conditions(n_per_batch)\n",
    "\n",
    "    for real, cond in zip(reals, conds):\n",
    "        plotcount += 1\n",
    "        # for each cond, make several predictions with different latent noise\n",
    "        latent = np.random.normal(size=(n_fake_per_real, latent_dim))\n",
    "        # for efficiency reason, we dont make a single forecast with the network, but\n",
    "        # we batch all n_fake_per_real together\n",
    "        cond_batch = np.repeat(cond[np.newaxis], repeats=n_fake_per_real, axis=0)\n",
    "        generated = gen.predict([latent, cond_batch])\n",
    "\n",
    "\n",
    "        # make a matrix of mapplots.\n",
    "        # first column: condition (daily mean), the same for every row\n",
    "        # first row: real fractions per hour\n",
    "        # rest of the rows: generated fractions per hour, 1 row per realization\n",
    "        fig = plt.figure(figsize=(25, 12))\n",
    "        n_plot = n_fake_per_real + 1\n",
    "        ax = plt.subplot(n_plot, 25, 1)\n",
    "        # compute unnormalized daily sum. squeeze away empty channel dimension (for plotting)\n",
    "        dsum = cond.squeeze() * norm_scale\n",
    "        plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "        plt.axis('off')\n",
    "        ax.annotate('real', xy=(0, 0.5), xytext=(-5, 0), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='right', va='center', rotation='vertical')\n",
    "        ax.annotate(f'daily sum', xy=(0.5, 1), xytext=(0, 5), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='center', va='baseline')\n",
    "        for jplot in range(1, 24 + 1):\n",
    "            ax = plt.subplot(n_plot, 25, jplot + 1)\n",
    "            plt.imshow(real[jplot - 1, :, :].squeeze(), vmin=0, vmax=1, cmap=plt.cm.Greys)\n",
    "            plt.axis('off')\n",
    "            ax.annotate(f'{jplot:02d}'':00', xy=(0.5, 1), xytext=(0, 5),\n",
    "                        xycoords='axes fraction', textcoords='offset points',\n",
    "                        size='large', ha='center', va='baseline')\n",
    "        # plot fake samples\n",
    "        for iplot in range(n_fake_per_real):\n",
    "            plt.subplot(n_plot, 25, (iplot + 1) * 25 + 1)\n",
    "            plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "            plt.axis('off')\n",
    "            for jplot in range(1, 24 + 1):\n",
    "                plt.subplot(n_plot, 25, (iplot + 1) * 25 + jplot + 1)\n",
    "                im = plt.imshow(generated[iplot, jplot - 1, :, :].squeeze(), vmin=0, vmax=1, cmap=plt.cm.Greys)\n",
    "                plt.axis('off')\n",
    "        fig.subplots_adjust(right=0.93)\n",
    "        cbar_ax = fig.add_axes([0.93, 0.15, 0.007, 0.7])\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('fraction of daily precipitation', fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "        plt.savefig(f'{plotdir}/generated_fractions_{params}_{epoch:04d}_{plotcount:04d}_allhours.{plot_format}')\n",
    "\n",
    "        # now the same, but showing absolute precipitation fields\n",
    "        # compute absolute precipitation from fraction of daily sum.\n",
    "        # this can be done with numpy broadcasting.\n",
    "        # we also have to multiply with norm_scale (because cond is normalized)\n",
    "        generated_scaled = generated * cond * norm_scale\n",
    "\n",
    "        real_scaled = real * cond * norm_scale\n",
    "        fig = plt.figure(figsize=(25, 12))\n",
    "        # plot real one\n",
    "        ax = plt.subplot(n_plot, 25, 1)\n",
    "        im = plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "        plt.axis('off')\n",
    "        ax.annotate('real', xy=(0, 0.5), xytext=(-5, 0), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='right', va='center', rotation='vertical')\n",
    "        ax.annotate(f'daily sum', xy=(0.5, 1), xytext=(0, 5), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='center', va='baseline')\n",
    "\n",
    "        for jplot in range(1, 24 + 1):\n",
    "            ax = plt.subplot(n_plot, 25, jplot + 1)\n",
    "            plt.imshow(real_scaled[jplot - 1, :, :].squeeze(), cmap=cmap, norm=plotnorm)\n",
    "            plt.axis('off')\n",
    "            ax.annotate(f'{jplot:02d}'':00', xy=(0.5, 1), xytext=(0, 5),\n",
    "                        xycoords='axes fraction', textcoords='offset points',\n",
    "                        size='large', ha='center', va='baseline')\n",
    "        # plot fake samples\n",
    "        for iplot in range(n_fake_per_real):\n",
    "            plt.subplot(n_plot, 25, (iplot + 1) * 25 + 1)\n",
    "            plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "            plt.axis('off')\n",
    "            for jplot in range(1, 24 + 1):\n",
    "                plt.subplot(n_plot, 25, (iplot + 1) * 25 + jplot + 1)\n",
    "                plt.imshow(generated_scaled[iplot, jplot - 1, :, :].squeeze(), cmap=cmap, norm=plotnorm)\n",
    "                plt.axis('off')\n",
    "        fig.subplots_adjust(right=0.93)\n",
    "        cbar_ax = fig.add_axes([0.93, 0.15, 0.007, 0.7])\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('precipitation [mm]', fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=16)\n",
    "        plt.savefig(f'{plotdir}/generated_precip_{params}_{epoch:04d}_{plotcount:04d}_allhours.{plot_format}')\n",
    "\n",
    "        np.save(f'data/real_precip_for_mapplots_{plotcount}.npy', real_scaled)\n",
    "\n",
    "        # same as before, but only every 3rd hour.\n",
    "        # rest of the rows: generated fractions per 3rd hour, 1 row per realization\n",
    "        fig = plt.figure(figsize=(12, 12))\n",
    "        n_plot = n_fake_per_real + 1\n",
    "        ax = plt.subplot(n_plot, 9, 1)\n",
    "        # compute unnormalized daily sum. squeeze away empty channel dimension (for plotting)\n",
    "        dsum = cond.squeeze() * norm_scale\n",
    "        plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "        plt.axis('off')\n",
    "        ax.annotate('real', xy=(0, 0.5), xytext=(-5, 0), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='right', va='center', rotation='vertical')\n",
    "        ax.annotate(f'daily sum', xy=(0.5, 1), xytext=(0, 5), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='center', va='baseline')\n",
    "        for jplot in range(1, 8 + 1):\n",
    "            ax = plt.subplot(n_plot, 9, jplot + 1)\n",
    "            plt.imshow(real[jplot*3 - 1, :, :].squeeze(), vmin=0, vmax=1, cmap=plt.cm.Greys)\n",
    "            plt.axis('off')\n",
    "            hour = jplot*3\n",
    "            ax.annotate(f'{hour:02d}'':00', xy=(0.5, 1), xytext=(0, 5),\n",
    "                        xycoords='axes fraction', textcoords='offset points',\n",
    "                        size='large', ha='center', va='baseline')\n",
    "        # plot fake samples\n",
    "        for iplot in range(n_fake_per_real):\n",
    "            plt.subplot(n_plot, 8+1, (iplot + 1) * 9 + 1)\n",
    "            plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "            plt.axis('off')\n",
    "            for jplot in range(1, 8 + 1):\n",
    "                plt.subplot(n_plot, 9, (iplot + 1) * 9 + jplot + 1)\n",
    "                im = plt.imshow(generated[iplot, jplot*3 - 1, :, :].squeeze(), vmin=0, vmax=1, cmap=plt.cm.Greys)\n",
    "                plt.axis('off')\n",
    "        fig.subplots_adjust(right=0.93)\n",
    "        cbar_ax = fig.add_axes([0.93, 0.15, 0.007, 0.7])\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('fraction of daily precipitation', fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "        plt.savefig(f'{plotdir}/generated_fractions_{params}_{epoch:04d}_{plotcount:04d}.{plot_format}')\n",
    "\n",
    "        # now the same, but showing absolute precipitation fields\n",
    "        # compute absolute precipitation from fraction of daily sum.\n",
    "        # this can be done with numpy broadcasting.\n",
    "        # we also have to multiply with norm_scale (because cond is normalized)\n",
    "        generated_scaled = generated * cond * norm_scale\n",
    "\n",
    "        real_scaled = real * cond * norm_scale\n",
    "        fig = plt.figure(figsize=(12, 12))\n",
    "        n_plot = n_fake_per_real + 1\n",
    "        ax = plt.subplot(n_plot, 9, 1)\n",
    "        # compute unnormalized daily sum. squeeze away empty channel dimension (for plotting)\n",
    "        dsum = cond.squeeze() * norm_scale\n",
    "        plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "        plt.axis('off')\n",
    "        ax.annotate('real', xy=(0, 0.5), xytext=(-5, 0), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='right', va='center', rotation='vertical')\n",
    "        ax.annotate(f'daily sum', xy=(0.5, 1), xytext=(0, 5), xycoords='axes fraction', textcoords='offset points',\n",
    "                    size='large', ha='center', va='baseline')\n",
    "        for jplot in range(1, 8 + 1):\n",
    "            ax = plt.subplot(n_plot, 9, jplot + 1)\n",
    "            plt.imshow(real_scaled[jplot*3 - 1, :, :].squeeze(), cmap=cmap, norm=plotnorm)\n",
    "            plt.axis('off')\n",
    "            hour = jplot*3\n",
    "            ax.annotate(f'{hour:02d}'':00', xy=(0.5, 1), xytext=(0, 5),\n",
    "                        xycoords='axes fraction', textcoords='offset points',\n",
    "                        size='large', ha='center', va='baseline')\n",
    "        # plot fake samples\n",
    "        for iplot in range(n_fake_per_real):\n",
    "            plt.subplot(n_plot, 8+1, (iplot + 1) * 9 + 1)\n",
    "            plt.imshow(dsum, cmap=cmap, norm=plotnorm)\n",
    "            plt.axis('off')\n",
    "            for jplot in range(1, 8 + 1):\n",
    "                plt.subplot(n_plot, 9, (iplot + 1) * 9 + jplot + 1)\n",
    "                im = plt.imshow(generated_scaled[iplot, jplot*3 - 1, :, :].squeeze(), cmap=cmap, norm=plotnorm)\n",
    "                plt.axis('off')\n",
    "        fig.subplots_adjust(right=0.93)\n",
    "        cbar_ax = fig.add_axes([0.93, 0.15, 0.007, 0.7])\n",
    "        cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "        cbar.set_label('precipitation [mm]', fontsize=16)\n",
    "        cbar.ax.tick_params(labelsize=16)\n",
    "        plt.savefig(f'{plotdir}/generated_precip_{params}_{epoch:04d}_{plotcount:04d}.{plot_format}')\n",
    "\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "# compute statistics over\n",
    "# many generated smaples\n",
    "# we compute the areamean,\n",
    "n_sample = 10000\n",
    "amean_fraction_gen = []\n",
    "amean_fraction_real = []\n",
    "amean_gen = []\n",
    "amean_real = []\n",
    "dists_real = []\n",
    "dists_gen = []\n",
    "\n",
    "\n",
    "# for each real conditoin, we crate 1 fake sample\n",
    "for i in trange(n_sample):\n",
    "    real, cond = generate_real_samples_and_conditions(1)\n",
    "    latent = np.random.normal(size=(1, latent_dim))\n",
    "    generated = gen.predict([latent, cond])\n",
    "\n",
    "    generated = generated.squeeze()\n",
    "    real = real.squeeze()\n",
    "    cond = cond.squeeze()\n",
    "    # compute area means\n",
    "    amean_fraction_gen.append(np.mean(generated, axis=(1, 2)).squeeze())\n",
    "    amean_fraction_real.append(np.mean(real, axis=(1, 2)).squeeze())\n",
    "    amean_gen.append(np.mean(generated * cond * norm_scale, axis=(1, 2)).squeeze())\n",
    "    amean_real.append(np.mean(real * cond * norm_scale, axis=(1, 2)).squeeze())\n",
    "    dists_real.append(real * cond * norm_scale)\n",
    "    dists_gen.append(generated * cond * norm_scale)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "amean_fraction_gen = np.array(amean_fraction_gen)\n",
    "amean_fraction_real = np.array(amean_fraction_real)\n",
    "amean_gen = np.array(amean_gen)\n",
    "amean_real = np.array(amean_real)\n",
    "dists_gen = np.array(dists_gen)\n",
    "dists_real = np.array(dists_real)\n",
    "np.save('data/generated_samples.npy',dists_gen)\n",
    "np.save('data/real_samples.npy', dists_real)\n",
    "\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)\n",
    "    n = x.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    return(x, y)\n",
    "\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "# ecdf of area means. the hours are flattened\n",
    "plt.figure()\n",
    "ax1 = plt.subplot(211)\n",
    "plt.plot(*ecdf(amean_gen.flatten()), label='gen')\n",
    "plt.plot(*ecdf(amean_real.flatten()), label='real')\n",
    "plt.legend(loc='upper left')\n",
    "sns.despine()\n",
    "plt.xlabel('mm/h')\n",
    "plt.ylabel('ecdf areamean')\n",
    "plt.semilogx()\n",
    "# ecdf of (flattened) spatial data\n",
    "ax2 = plt.subplot(212)\n",
    "plt.plot(*ecdf(dists_gen.flatten()), label='gen')\n",
    "plt.plot(*ecdf(dists_real.flatten()), label='real')\n",
    "plt.legend(loc='upper left')\n",
    "sns.despine()\n",
    "plt.ylabel('ecdf')\n",
    "plt.xlabel('mm/h')\n",
    "plt.semilogx()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{plotdir}/ecdf_allx_{params}_{epoch:04d}.png', dpi=400)\n",
    "# cut at 0.1mm/h\n",
    "ax1.set_xlim(xmin=0.5)\n",
    "ax1.set_ylim(ymin=0.8, ymax=1.01)\n",
    "ax2.set_xlim(xmin=0.1)\n",
    "ax2.set_ylim(ymin=0.6, ymax=1.01)\n",
    "plt.savefig(f'{plotdir}/ecdf_{params}_{epoch:04d}.png', dpi=400)\n",
    "\n",
    "plt.close('all')\n",
    "# free some memory\n",
    "del dists_gen\n",
    "del dists_real\n",
    "\n",
    "# convert to pandas data frame, with timeofday ('hour') as additional column\n",
    "res_df = []\n",
    "for i in range(24):\n",
    "    _df1 = pd.DataFrame({'fraction': amean_fraction_gen[:, i],\n",
    "                         'precip': amean_gen[:, i],\n",
    "                         'typ': 'generated',\n",
    "                         'hour': i + 1}, index=np.arange(len(amean_gen)))\n",
    "    _df2 = pd.DataFrame({'fraction': amean_fraction_real[:, i].squeeze(),\n",
    "                         'precip': amean_real[:, i],\n",
    "                         'typ': 'real',\n",
    "                         'hour': i + 1}, index=np.arange(len(amean_gen)))\n",
    "    res_df.append(_df1)\n",
    "    res_df.append(_df2)\n",
    "\n",
    "\n",
    "df = pd.concat(res_df)\n",
    "df.to_csv(f'{plotdir}/gen_and_real_ameans_{params}_{epoch:04d}.csv')\n",
    "\n",
    "# make boxplot\n",
    "for showfliers in (True, False):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(211)\n",
    "    sns.boxplot('hour', 'precip', data=df, hue='typ', showfliers=showfliers)\n",
    "    plt.xlabel('')\n",
    "    sns.despine()\n",
    "    plt.subplot(212)\n",
    "    sns.boxplot('hour', 'fraction', data=df, hue='typ', showfliers=showfliers)\n",
    "    sns.despine()\n",
    "    plt.suptitle(f'n={n_sample}')\n",
    "    plt.savefig(f'{plotdir}/daily_cycle_showfliers{showfliers}_{params}_{epoch:04d}.svg')\n",
    "\n",
    "\n",
    "## for a single real one, generate a large\n",
    "# number of fake distributions, and then\n",
    "# plot the areamean in a lineplot\n",
    "# we generate 100 fake distributions with different noise accross the samples\n",
    "# and additionally 10 fake ones that use the same noise for all plots\n",
    "# the latter we plot in the same color (1 seperate color for each generated one)\n",
    "# so that we can compare them accross the plots\n",
    "\n",
    "n_to_generate = 20\n",
    "n_fake_per_real = 100\n",
    "n_fake_per_real_samenoise = 10\n",
    "plotcount = 0\n",
    "hours = np.arange(1, 24 + 1)\n",
    "# use same noise for all samples\n",
    "latent_shared = np.random.normal(size=(n_fake_per_real_samenoise, latent_dim))\n",
    "for isample in trange(n_to_generate):\n",
    "    real, cond = generate_real_samples_and_conditions(1)\n",
    "    latent= np.random.normal(size=(n_fake_per_real, latent_dim))\n",
    "    # for efficiency reason, we dont make a single forecast with the network, but\n",
    "    # we batch all n_fake_per_real together\n",
    "    cond_batch = np.repeat(cond, repeats=n_fake_per_real, axis=0)\n",
    "    cond_batch_samenoise = np.repeat(cond, repeats=n_fake_per_real_samenoise, axis=0)\n",
    "    generated = gen.predict([latent, cond_batch], verbose=1)\n",
    "    generated_samenoise = gen.predict([latent_shared, cond_batch_samenoise], verbose=1)\n",
    "    real = real.squeeze()\n",
    "    generated = generated.squeeze()\n",
    "    generated_samenoise = generated_samenoise.squeeze()\n",
    "    # compute are mean\n",
    "    amean_real = np.mean(real * cond.squeeze() * norm_scale, (1, 2))\n",
    "    amean_gen = np.mean(generated * cond.squeeze() * norm_scale, (2, 3))  # generated has a time dimension\n",
    "    amean_gen_samenoise = np.mean(generated_samenoise * cond.squeeze() * norm_scale, (2, 3))  # generated has a time dimension\n",
    "\n",
    "    plt.figure(figsize=(7, 3))\n",
    "    plt.plot(hours, amean_gen.T, label='_nolegend_', alpha=0.3,color='#1b9e77')\n",
    "    plt.plot(hours, amean_gen_samenoise.T, label='_nolegend_', alpha=1)\n",
    "    plt.plot(hours, amean_real, label='real', color='black')\n",
    "    plt.xlabel('hour')\n",
    "    plt.ylabel('precipitation [mm/hour]')\n",
    "    plt.legend()\n",
    "    sns.despine()\n",
    "    plt.savefig(f'{plotdir}/distribution_lineplot_samenosie_{params}_{epoch:04d}_{isample:04d}.svg')\n",
    "    plt.close('all')\n",
    "\n",
    "# take two conditions, and\n",
    "# then plot the areamean of the resulting distributions, and check whether they are different\n",
    "# we use the same noise for both, to avoid finding effects that only might come from the noise\n",
    "n_fake_per_real = 1000\n",
    "latent = np.random.normal(size=(n_fake_per_real, latent_dim))\n",
    "for isample in trange(20):\n",
    "    real1, cond1 = generate_real_samples_and_conditions(1)\n",
    "\n",
    "    cond_batch1 = np.repeat(cond1, repeats=n_fake_per_real, axis=0)\n",
    "    generated1 = gen.predict([latent, cond_batch1], verbose=1)\n",
    "    real2, cond2 = generate_real_samples_and_conditions(1)\n",
    "    cond_batch2 = np.repeat(cond2, repeats=n_fake_per_real, axis=0)\n",
    "    generated2 = gen.predict([latent, cond_batch2], verbose=1)\n",
    "\n",
    "    amean_fraction_real1 = np.mean(real1.squeeze(), (1, 2)).squeeze()\n",
    "    amean_fraction_gen1 = np.mean(generated1, (2, 3)).squeeze()  # generated has a time dimension\n",
    "    amean_fraction_real2 = np.mean(real2.squeeze(), (1, 2)).squeeze()\n",
    "    amean_fraction_gen2 = np.mean(generated2.squeeze(), (2, 3)).squeeze()  # generated has a time dimension\n",
    "\n",
    "    res_df = []\n",
    "    for i in range(24):\n",
    "        _df1 = pd.DataFrame({'fraction': amean_fraction_gen1[:, i],\n",
    "                             'cond': 1,\n",
    "                             'hour': i + 1}, index=np.arange(len(amean_fraction_gen1)))\n",
    "        _df2 = pd.DataFrame({'fraction': amean_fraction_gen2[:, i],\n",
    "                             'cond': 2,\n",
    "                             'hour': i + 1}, index=np.arange(len(amean_fraction_gen1)))\n",
    "        res_df.append(_df1)\n",
    "        res_df.append(_df2)\n",
    "\n",
    "    df = pd.concat(res_df)\n",
    "    df.to_csv(f'{plotdir}/check_conditional_dist_samenoise_{params}_{epoch:04d}_{isample:04d}.csv')\n",
    "    pvals_per_hour = []\n",
    "    for hour in range(1,24+1):\n",
    "        sub = df.query('hour==@hour')\n",
    "        _, p = scipy.stats.ks_2samp(sub.query('cond==1')['fraction'], sub.query('cond==2')['fraction'])\n",
    "        pvals_per_hour.append(p)\n",
    "    np.savetxt(f'{plotdir}/check_conditional_dist_samenoise_KSpval{params}_{epoch:04d}_{isample:04d}.txt', pvals_per_hour)\n",
    "    for showfliers in (True, False):\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(6, 4.8))\n",
    "        gs = fig.add_gridspec(2, 2)\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        im = ax1.imshow(cond1.squeeze(), cmap=cmap, norm=plotnorm)\n",
    "        plt.title('cond 1')\n",
    "        plt.axis('off')\n",
    "        plt.colorbar(im)\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        im = ax2.imshow(cond2.squeeze(), cmap=cmap, norm=plotnorm)\n",
    "        plt.title('cond 2')\n",
    "        plt.axis('off')\n",
    "        plt.colorbar(im)\n",
    "        ax3 = fig.add_subplot(gs[1, :])\n",
    "        sns.boxplot('hour', 'fraction', hue='cond', data=df, ax=ax3, showfliers=showfliers)\n",
    "        sns.despine()\n",
    "        plt.savefig(f'{plotdir}/check_conditional_dist_samenoise_showfliers{showfliers}_{params}_{epoch:04d}_{isample:04d}.svg')\n",
    "\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377f11f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
